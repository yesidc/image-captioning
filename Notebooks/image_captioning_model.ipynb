{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7226c3a",
   "metadata": {},
   "source": [
    "## Image-captioning model\n",
    "\n",
    "### Overview\n",
    "\n",
    "The code below trains an image captioning algorithm using a pre-trained vision and language transformer as well as the COCO dataset. The model uses the Microsoft Swin-Tiny-Patch4-Window7-224 and DistilGPT-2  pre-trained models as the encoder and decoder respectively.\n",
    "The code implementation is done with the help of the following libraries/packages:\n",
    " - Transformers library from Hugging Face to implement the model.\n",
    " - PyTorch for tensor operations\n",
    " - PIL and torchvision libraries for image processing.\n",
    "\n",
    "\n",
    "### Instructions to Run this notebook\n",
    "\n",
    "To run this code on a local machine, follow these steps:\n",
    "\n",
    "- Install the requirements listed in `requirements.txt`. Make sure to include the Transformers, datasets, scikit-learn, PIL, and PyTorch libraries/packages\n",
    "\n",
    "- Download the COCO dataset and place it in the directory specified by the COCO_DIR variable.\n",
    "\n",
    "Run the code in a Python environment that has access to a GPU. The code can be run in a Jupyter notebook or in a Python script. This code was run on MacBook M1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41994dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from transformers import VisionEncoderDecoderModel, GPT2TokenizerFast, AutoFeatureExtractor, \\\n",
    "                          TrainingArguments, Trainer\n",
    "\n",
    "import datasets\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4167cac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yesidcano/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1673856676759/work/aten/src/ATen/native/TensorShape.cpp:3454.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Some weights of the model checkpoint at microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['transformer.h.3.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.3.crossattention.bias', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.4.crossattention.masked_bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.2.crossattention.masked_bias', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.1.crossattention.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.1.crossattention.masked_bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.0.crossattention.masked_bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.5.crossattention.masked_bias', 'transformer.h.3.crossattention.masked_bias', 'transformer.h.2.crossattention.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.0.crossattention.bias', 'transformer.h.5.crossattention.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.4.crossattention.bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model uses a rre-trained encoder of type <class 'transformers.models.swin.modeling_swin.SwinModel'> and pre-trained decoder of type <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>\n"
     ]
    }
   ],
   "source": [
    "# ???Many weights are initialized randomly, namely the cross attention weights??? change this comment. not mine\n",
    "#One of the main objectives is optimizing the cross-attention weights. How GPT connects to the encoders output,\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    'microsoft/swin-tiny-patch4-window7-224',\n",
    "    'distilgpt2'\n",
    ")\n",
    "\n",
    "print(f'This model uses a rre-trained encoder of type {type(model.encoder)} and pre-trained decoder of type {type(model.decoder)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Device\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "RadioButtons(description='Select device', options=('M1', 'Other'), value='M1')",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e8445449653d4f2fa439ed521719e7e8"
      }
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_type = widgets.RadioButtons(\n",
    "    options=['M1', 'Other'],\n",
    "    value='M1',\n",
    "    description='Select device',\n",
    "    disabled=False\n",
    ")\n",
    "device_type"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "VisionEncoderDecoderModel(\n  (encoder): SwinModel(\n    (embeddings): SwinEmbeddings(\n      (patch_embeddings): SwinPatchEmbeddings(\n        (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n      )\n      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): SwinEncoder(\n      (layers): ModuleList(\n        (0): SwinStage(\n          (blocks): ModuleList(\n            (0-1): 2 x SwinLayer(\n              (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=96, out_features=96, bias=True)\n                  (key): Linear(in_features=96, out_features=96, bias=True)\n                  (value): Linear(in_features=96, out_features=96, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=96, out_features=96, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.1)\n              (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=96, out_features=384, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=384, out_features=96, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n          )\n          (downsample): SwinPatchMerging(\n            (reduction): Linear(in_features=384, out_features=192, bias=False)\n            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (1): SwinStage(\n          (blocks): ModuleList(\n            (0-1): 2 x SwinLayer(\n              (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=192, out_features=192, bias=True)\n                  (key): Linear(in_features=192, out_features=192, bias=True)\n                  (value): Linear(in_features=192, out_features=192, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=192, out_features=192, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.1)\n              (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=192, out_features=768, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=768, out_features=192, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n          )\n          (downsample): SwinPatchMerging(\n            (reduction): Linear(in_features=768, out_features=384, bias=False)\n            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (2): SwinStage(\n          (blocks): ModuleList(\n            (0-5): 6 x SwinLayer(\n              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=384, out_features=384, bias=True)\n                  (key): Linear(in_features=384, out_features=384, bias=True)\n                  (value): Linear(in_features=384, out_features=384, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=384, out_features=384, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.1)\n              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=384, out_features=1536, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=1536, out_features=384, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n          )\n          (downsample): SwinPatchMerging(\n            (reduction): Linear(in_features=1536, out_features=768, bias=False)\n            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (3): SwinStage(\n          (blocks): ModuleList(\n            (0-1): 2 x SwinLayer(\n              (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=768, out_features=768, bias=True)\n                  (key): Linear(in_features=768, out_features=768, bias=True)\n                  (value): Linear(in_features=768, out_features=768, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.1)\n              (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n          )\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (pooler): AdaptiveAvgPool1d(output_size=1)\n  )\n  (decoder): GPT2LMHeadModel(\n    (transformer): GPT2Model(\n      (wte): Embedding(50257, 768)\n      (wpe): Embedding(1024, 768)\n      (drop): Dropout(p=0.1, inplace=False)\n      (h): ModuleList(\n        (0-5): 6 x GPT2Block(\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): GPT2Attention(\n            (c_attn): Conv1D()\n            (c_proj): Conv1D()\n            (attn_dropout): Dropout(p=0.1, inplace=False)\n            (resid_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (crossattention): GPT2Attention(\n            (c_attn): Conv1D()\n            (q_attn): Conv1D()\n            (c_proj): Conv1D()\n            (attn_dropout): Dropout(p=0.1, inplace=False)\n            (resid_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): GPT2MLP(\n            (c_fc): Conv1D()\n            (c_proj): Conv1D()\n            (act): NewGELUActivation()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n  )\n)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train on gpu\n",
    "if device_type.value == 'M1':\n",
    "\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4e9f1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 123,615,354\n"
     ]
    }
   ],
   "source": [
    "# Vit and GPT2 have 182,485,248 combined parameters. Swin version I am tuning here has fewer parameters\n",
    "from torch import numel\n",
    "\n",
    "combined_params = 0\n",
    "for param in model.parameters():\n",
    "    combined_params += numel(param)\n",
    "    \n",
    "print(f\"Total number of parameters: {combined_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Database\n",
    "The image captioning algorithm is trained on the COCO dataset which consists of images and captions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# COCO_DIR = input('Path to COCO dataset')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset coco_dataset_script (/Users/yesidcano/.cache/huggingface/datasets/ydshieh___coco_dataset_script/2017-6b5176efb5303df4/0.0.0/e033205c0266a54c10be132f9264f2a39dcf893e798f6756d224b1ff5078998f)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['image_id', 'caption_id', 'caption', 'height', 'width', 'file_name', 'coco_url', 'image_path'],\n    num_rows: 990\n})"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the datasets.load_dataset manages everything related to caching. So I have to use it.\n",
    "COCO_DIR = '/Users/yesidcano/repos/image-captioning/data/coco'\n",
    "\n",
    "\n",
    "#ds = datasets.load_dataset(\"ydshieh/coco_dataset_script\", \"2017\",data_dir=COCO_DIR, cache_dir='/Users/yesidcano/repos/db_coco_cache')\n",
    "\n",
    "# Load a slice of the database this https://huggingface.co/docs/datasets/loading to split dataset.\n",
    "\n",
    "ds = datasets.load_dataset(\"ydshieh/coco_dataset_script\", \"2017\",data_dir=COCO_DIR, split=\"train[10:1000]\")\n",
    "ds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4450a9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
    "#gpt-2 tokenizer\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('distilgpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Model config\n",
    "\n",
    "model.config.pad_token = tokenizer.pad_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "model.config.decoder_start_token = tokenizer.bos_token\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "ViTFeatureExtractor {\n  \"do_normalize\": true,\n  \"do_resize\": true,\n  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n  \"image_mean\": [\n    0.485,\n    0.456,\n    0.406\n  ],\n  \"image_std\": [\n    0.229,\n    0.224,\n    0.225\n  ],\n  \"resample\": 3,\n  \"size\": 224\n}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "PreTrainedTokenizerFast(name_or_path='distilgpt2', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/990 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "13da59f991ab4bf697cc47202ed11fcb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Define the transforms to be applied to the images\n",
    "transform = transforms.Compose([\n",
    "\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "])\n",
    "\n",
    "def preprocess_fn(examples):\n",
    "\n",
    "    # Swin expects pixel_values instead of input_ids\n",
    "    examples['pixel_values'] = [transform(Image.open(path).convert('RGB')) for path in examples['image_path']]\n",
    "    # We are padding tokens here instead of using a datacollator\n",
    "    tokenized = tokenizer(\n",
    "        examples['caption'], padding='max_length', max_length=10, truncation=True\n",
    "    )['input_ids']\n",
    "    # the output captions\n",
    "    examples['labels'] = [[l if l != tokenizer.pad_token_id else -100 for l in t] for t in tokenized]\n",
    "\n",
    "    # delete unused keys\n",
    "    del examples['image_path']\n",
    "    del examples['caption']\n",
    "    return examples\n",
    "\n",
    "processed_dataset = ds.map(\n",
    "    function=preprocess_fn,\n",
    "    batched=True,\n",
    "    batch_size = 50,\n",
    "    #remove_columns=['image_id', 'caption_id', 'caption', 'height', 'width', 'file_name', 'coco_url', 'image_path']\n",
    "\n",
    "\n",
    "\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5ca503c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['image_id', 'caption_id', 'height', 'width', 'file_name', 'coco_url', 'pixel_values', 'labels'],\n        num_rows: 891\n    })\n    test: Dataset({\n        features: ['image_id', 'caption_id', 'height', 'width', 'file_name', 'coco_url', 'pixel_values', 'labels'],\n        num_rows: 99\n    })\n})"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By default data are shuffled.\n",
    "processed_dataset = processed_dataset.train_test_split(test_size=0.1)\n",
    "processed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model's layers\n",
    "Below a list of the encoder and decoder's layers respectively."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Encoder Layers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.patch_embeddings.projection.weight\n",
      "embeddings.patch_embeddings.projection.bias\n",
      "embeddings.norm.weight\n",
      "embeddings.norm.bias\n",
      "encoder.layers.0.blocks.0.layernorm_before.weight\n",
      "encoder.layers.0.blocks.0.layernorm_before.bias\n",
      "encoder.layers.0.blocks.0.attention.self.relative_position_bias_table\n",
      "encoder.layers.0.blocks.0.attention.self.query.weight\n",
      "encoder.layers.0.blocks.0.attention.self.query.bias\n",
      "encoder.layers.0.blocks.0.attention.self.key.weight\n",
      "encoder.layers.0.blocks.0.attention.self.key.bias\n",
      "encoder.layers.0.blocks.0.attention.self.value.weight\n",
      "encoder.layers.0.blocks.0.attention.self.value.bias\n",
      "encoder.layers.0.blocks.0.attention.output.dense.weight\n",
      "encoder.layers.0.blocks.0.attention.output.dense.bias\n",
      "encoder.layers.0.blocks.0.layernorm_after.weight\n",
      "encoder.layers.0.blocks.0.layernorm_after.bias\n",
      "encoder.layers.0.blocks.0.intermediate.dense.weight\n",
      "encoder.layers.0.blocks.0.intermediate.dense.bias\n",
      "encoder.layers.0.blocks.0.output.dense.weight\n",
      "encoder.layers.0.blocks.0.output.dense.bias\n",
      "encoder.layers.0.blocks.1.layernorm_before.weight\n",
      "encoder.layers.0.blocks.1.layernorm_before.bias\n",
      "encoder.layers.0.blocks.1.attention.self.relative_position_bias_table\n",
      "encoder.layers.0.blocks.1.attention.self.query.weight\n",
      "encoder.layers.0.blocks.1.attention.self.query.bias\n",
      "encoder.layers.0.blocks.1.attention.self.key.weight\n",
      "encoder.layers.0.blocks.1.attention.self.key.bias\n",
      "encoder.layers.0.blocks.1.attention.self.value.weight\n",
      "encoder.layers.0.blocks.1.attention.self.value.bias\n",
      "encoder.layers.0.blocks.1.attention.output.dense.weight\n",
      "encoder.layers.0.blocks.1.attention.output.dense.bias\n",
      "encoder.layers.0.blocks.1.layernorm_after.weight\n",
      "encoder.layers.0.blocks.1.layernorm_after.bias\n",
      "encoder.layers.0.blocks.1.intermediate.dense.weight\n",
      "encoder.layers.0.blocks.1.intermediate.dense.bias\n",
      "encoder.layers.0.blocks.1.output.dense.weight\n",
      "encoder.layers.0.blocks.1.output.dense.bias\n",
      "encoder.layers.0.downsample.reduction.weight\n",
      "encoder.layers.0.downsample.norm.weight\n",
      "encoder.layers.0.downsample.norm.bias\n",
      "encoder.layers.1.blocks.0.layernorm_before.weight\n",
      "encoder.layers.1.blocks.0.layernorm_before.bias\n",
      "encoder.layers.1.blocks.0.attention.self.relative_position_bias_table\n",
      "encoder.layers.1.blocks.0.attention.self.query.weight\n",
      "encoder.layers.1.blocks.0.attention.self.query.bias\n",
      "encoder.layers.1.blocks.0.attention.self.key.weight\n",
      "encoder.layers.1.blocks.0.attention.self.key.bias\n",
      "encoder.layers.1.blocks.0.attention.self.value.weight\n",
      "encoder.layers.1.blocks.0.attention.self.value.bias\n",
      "encoder.layers.1.blocks.0.attention.output.dense.weight\n",
      "encoder.layers.1.blocks.0.attention.output.dense.bias\n",
      "encoder.layers.1.blocks.0.layernorm_after.weight\n",
      "encoder.layers.1.blocks.0.layernorm_after.bias\n",
      "encoder.layers.1.blocks.0.intermediate.dense.weight\n",
      "encoder.layers.1.blocks.0.intermediate.dense.bias\n",
      "encoder.layers.1.blocks.0.output.dense.weight\n",
      "encoder.layers.1.blocks.0.output.dense.bias\n",
      "encoder.layers.1.blocks.1.layernorm_before.weight\n",
      "encoder.layers.1.blocks.1.layernorm_before.bias\n",
      "encoder.layers.1.blocks.1.attention.self.relative_position_bias_table\n",
      "encoder.layers.1.blocks.1.attention.self.query.weight\n",
      "encoder.layers.1.blocks.1.attention.self.query.bias\n",
      "encoder.layers.1.blocks.1.attention.self.key.weight\n",
      "encoder.layers.1.blocks.1.attention.self.key.bias\n",
      "encoder.layers.1.blocks.1.attention.self.value.weight\n",
      "encoder.layers.1.blocks.1.attention.self.value.bias\n",
      "encoder.layers.1.blocks.1.attention.output.dense.weight\n",
      "encoder.layers.1.blocks.1.attention.output.dense.bias\n",
      "encoder.layers.1.blocks.1.layernorm_after.weight\n",
      "encoder.layers.1.blocks.1.layernorm_after.bias\n",
      "encoder.layers.1.blocks.1.intermediate.dense.weight\n",
      "encoder.layers.1.blocks.1.intermediate.dense.bias\n",
      "encoder.layers.1.blocks.1.output.dense.weight\n",
      "encoder.layers.1.blocks.1.output.dense.bias\n",
      "encoder.layers.1.downsample.reduction.weight\n",
      "encoder.layers.1.downsample.norm.weight\n",
      "encoder.layers.1.downsample.norm.bias\n",
      "encoder.layers.2.blocks.0.layernorm_before.weight\n",
      "encoder.layers.2.blocks.0.layernorm_before.bias\n",
      "encoder.layers.2.blocks.0.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.0.attention.self.query.weight\n",
      "encoder.layers.2.blocks.0.attention.self.query.bias\n",
      "encoder.layers.2.blocks.0.attention.self.key.weight\n",
      "encoder.layers.2.blocks.0.attention.self.key.bias\n",
      "encoder.layers.2.blocks.0.attention.self.value.weight\n",
      "encoder.layers.2.blocks.0.attention.self.value.bias\n",
      "encoder.layers.2.blocks.0.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.0.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.0.layernorm_after.weight\n",
      "encoder.layers.2.blocks.0.layernorm_after.bias\n",
      "encoder.layers.2.blocks.0.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.0.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.0.output.dense.weight\n",
      "encoder.layers.2.blocks.0.output.dense.bias\n",
      "encoder.layers.2.blocks.1.layernorm_before.weight\n",
      "encoder.layers.2.blocks.1.layernorm_before.bias\n",
      "encoder.layers.2.blocks.1.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.1.attention.self.query.weight\n",
      "encoder.layers.2.blocks.1.attention.self.query.bias\n",
      "encoder.layers.2.blocks.1.attention.self.key.weight\n",
      "encoder.layers.2.blocks.1.attention.self.key.bias\n",
      "encoder.layers.2.blocks.1.attention.self.value.weight\n",
      "encoder.layers.2.blocks.1.attention.self.value.bias\n",
      "encoder.layers.2.blocks.1.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.1.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.1.layernorm_after.weight\n",
      "encoder.layers.2.blocks.1.layernorm_after.bias\n",
      "encoder.layers.2.blocks.1.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.1.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.1.output.dense.weight\n",
      "encoder.layers.2.blocks.1.output.dense.bias\n",
      "encoder.layers.2.blocks.2.layernorm_before.weight\n",
      "encoder.layers.2.blocks.2.layernorm_before.bias\n",
      "encoder.layers.2.blocks.2.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.2.attention.self.query.weight\n",
      "encoder.layers.2.blocks.2.attention.self.query.bias\n",
      "encoder.layers.2.blocks.2.attention.self.key.weight\n",
      "encoder.layers.2.blocks.2.attention.self.key.bias\n",
      "encoder.layers.2.blocks.2.attention.self.value.weight\n",
      "encoder.layers.2.blocks.2.attention.self.value.bias\n",
      "encoder.layers.2.blocks.2.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.2.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.2.layernorm_after.weight\n",
      "encoder.layers.2.blocks.2.layernorm_after.bias\n",
      "encoder.layers.2.blocks.2.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.2.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.2.output.dense.weight\n",
      "encoder.layers.2.blocks.2.output.dense.bias\n",
      "encoder.layers.2.blocks.3.layernorm_before.weight\n",
      "encoder.layers.2.blocks.3.layernorm_before.bias\n",
      "encoder.layers.2.blocks.3.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.3.attention.self.query.weight\n",
      "encoder.layers.2.blocks.3.attention.self.query.bias\n",
      "encoder.layers.2.blocks.3.attention.self.key.weight\n",
      "encoder.layers.2.blocks.3.attention.self.key.bias\n",
      "encoder.layers.2.blocks.3.attention.self.value.weight\n",
      "encoder.layers.2.blocks.3.attention.self.value.bias\n",
      "encoder.layers.2.blocks.3.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.3.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.3.layernorm_after.weight\n",
      "encoder.layers.2.blocks.3.layernorm_after.bias\n",
      "encoder.layers.2.blocks.3.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.3.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.3.output.dense.weight\n",
      "encoder.layers.2.blocks.3.output.dense.bias\n",
      "encoder.layers.2.blocks.4.layernorm_before.weight\n",
      "encoder.layers.2.blocks.4.layernorm_before.bias\n",
      "encoder.layers.2.blocks.4.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.4.attention.self.query.weight\n",
      "encoder.layers.2.blocks.4.attention.self.query.bias\n",
      "encoder.layers.2.blocks.4.attention.self.key.weight\n",
      "encoder.layers.2.blocks.4.attention.self.key.bias\n",
      "encoder.layers.2.blocks.4.attention.self.value.weight\n",
      "encoder.layers.2.blocks.4.attention.self.value.bias\n",
      "encoder.layers.2.blocks.4.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.4.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.4.layernorm_after.weight\n",
      "encoder.layers.2.blocks.4.layernorm_after.bias\n",
      "encoder.layers.2.blocks.4.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.4.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.4.output.dense.weight\n",
      "encoder.layers.2.blocks.4.output.dense.bias\n",
      "encoder.layers.2.blocks.5.layernorm_before.weight\n",
      "encoder.layers.2.blocks.5.layernorm_before.bias\n",
      "encoder.layers.2.blocks.5.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.5.attention.self.query.weight\n",
      "encoder.layers.2.blocks.5.attention.self.query.bias\n",
      "encoder.layers.2.blocks.5.attention.self.key.weight\n",
      "encoder.layers.2.blocks.5.attention.self.key.bias\n",
      "encoder.layers.2.blocks.5.attention.self.value.weight\n",
      "encoder.layers.2.blocks.5.attention.self.value.bias\n",
      "encoder.layers.2.blocks.5.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.5.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.5.layernorm_after.weight\n",
      "encoder.layers.2.blocks.5.layernorm_after.bias\n",
      "encoder.layers.2.blocks.5.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.5.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.5.output.dense.weight\n",
      "encoder.layers.2.blocks.5.output.dense.bias\n",
      "encoder.layers.2.downsample.reduction.weight\n",
      "encoder.layers.2.downsample.norm.weight\n",
      "encoder.layers.2.downsample.norm.bias\n",
      "encoder.layers.3.blocks.0.layernorm_before.weight\n",
      "encoder.layers.3.blocks.0.layernorm_before.bias\n",
      "encoder.layers.3.blocks.0.attention.self.relative_position_bias_table\n",
      "encoder.layers.3.blocks.0.attention.self.query.weight\n",
      "encoder.layers.3.blocks.0.attention.self.query.bias\n",
      "encoder.layers.3.blocks.0.attention.self.key.weight\n",
      "encoder.layers.3.blocks.0.attention.self.key.bias\n",
      "encoder.layers.3.blocks.0.attention.self.value.weight\n",
      "encoder.layers.3.blocks.0.attention.self.value.bias\n",
      "encoder.layers.3.blocks.0.attention.output.dense.weight\n",
      "encoder.layers.3.blocks.0.attention.output.dense.bias\n",
      "encoder.layers.3.blocks.0.layernorm_after.weight\n",
      "encoder.layers.3.blocks.0.layernorm_after.bias\n",
      "encoder.layers.3.blocks.0.intermediate.dense.weight\n",
      "encoder.layers.3.blocks.0.intermediate.dense.bias\n",
      "encoder.layers.3.blocks.0.output.dense.weight\n",
      "encoder.layers.3.blocks.0.output.dense.bias\n",
      "encoder.layers.3.blocks.1.layernorm_before.weight\n",
      "encoder.layers.3.blocks.1.layernorm_before.bias\n",
      "encoder.layers.3.blocks.1.attention.self.relative_position_bias_table\n",
      "encoder.layers.3.blocks.1.attention.self.query.weight\n",
      "encoder.layers.3.blocks.1.attention.self.query.bias\n",
      "encoder.layers.3.blocks.1.attention.self.key.weight\n",
      "encoder.layers.3.blocks.1.attention.self.key.bias\n",
      "encoder.layers.3.blocks.1.attention.self.value.weight\n",
      "encoder.layers.3.blocks.1.attention.self.value.bias\n",
      "encoder.layers.3.blocks.1.attention.output.dense.weight\n",
      "encoder.layers.3.blocks.1.attention.output.dense.bias\n",
      "encoder.layers.3.blocks.1.layernorm_after.weight\n",
      "encoder.layers.3.blocks.1.layernorm_after.bias\n",
      "encoder.layers.3.blocks.1.intermediate.dense.weight\n",
      "encoder.layers.3.blocks.1.intermediate.dense.bias\n",
      "encoder.layers.3.blocks.1.output.dense.weight\n",
      "encoder.layers.3.blocks.1.output.dense.bias\n",
      "layernorm.weight\n",
      "layernorm.bias\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for name, param in model.encoder.named_parameters():\n",
    "    print(name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Decoder's layers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight\n",
      "transformer.wpe.weight\n",
      "transformer.h.0.ln_1.weight\n",
      "transformer.h.0.ln_1.bias\n",
      "transformer.h.0.attn.c_attn.weight\n",
      "transformer.h.0.attn.c_attn.bias\n",
      "transformer.h.0.attn.c_proj.weight\n",
      "transformer.h.0.attn.c_proj.bias\n",
      "transformer.h.0.ln_2.weight\n",
      "transformer.h.0.ln_2.bias\n",
      "transformer.h.0.crossattention.c_attn.weight\n",
      "transformer.h.0.crossattention.c_attn.bias\n",
      "transformer.h.0.crossattention.q_attn.weight\n",
      "transformer.h.0.crossattention.q_attn.bias\n",
      "transformer.h.0.crossattention.c_proj.weight\n",
      "transformer.h.0.crossattention.c_proj.bias\n",
      "transformer.h.0.ln_cross_attn.weight\n",
      "transformer.h.0.ln_cross_attn.bias\n",
      "transformer.h.0.mlp.c_fc.weight\n",
      "transformer.h.0.mlp.c_fc.bias\n",
      "transformer.h.0.mlp.c_proj.weight\n",
      "transformer.h.0.mlp.c_proj.bias\n",
      "transformer.h.1.ln_1.weight\n",
      "transformer.h.1.ln_1.bias\n",
      "transformer.h.1.attn.c_attn.weight\n",
      "transformer.h.1.attn.c_attn.bias\n",
      "transformer.h.1.attn.c_proj.weight\n",
      "transformer.h.1.attn.c_proj.bias\n",
      "transformer.h.1.ln_2.weight\n",
      "transformer.h.1.ln_2.bias\n",
      "transformer.h.1.crossattention.c_attn.weight\n",
      "transformer.h.1.crossattention.c_attn.bias\n",
      "transformer.h.1.crossattention.q_attn.weight\n",
      "transformer.h.1.crossattention.q_attn.bias\n",
      "transformer.h.1.crossattention.c_proj.weight\n",
      "transformer.h.1.crossattention.c_proj.bias\n",
      "transformer.h.1.ln_cross_attn.weight\n",
      "transformer.h.1.ln_cross_attn.bias\n",
      "transformer.h.1.mlp.c_fc.weight\n",
      "transformer.h.1.mlp.c_fc.bias\n",
      "transformer.h.1.mlp.c_proj.weight\n",
      "transformer.h.1.mlp.c_proj.bias\n",
      "transformer.h.2.ln_1.weight\n",
      "transformer.h.2.ln_1.bias\n",
      "transformer.h.2.attn.c_attn.weight\n",
      "transformer.h.2.attn.c_attn.bias\n",
      "transformer.h.2.attn.c_proj.weight\n",
      "transformer.h.2.attn.c_proj.bias\n",
      "transformer.h.2.ln_2.weight\n",
      "transformer.h.2.ln_2.bias\n",
      "transformer.h.2.crossattention.c_attn.weight\n",
      "transformer.h.2.crossattention.c_attn.bias\n",
      "transformer.h.2.crossattention.q_attn.weight\n",
      "transformer.h.2.crossattention.q_attn.bias\n",
      "transformer.h.2.crossattention.c_proj.weight\n",
      "transformer.h.2.crossattention.c_proj.bias\n",
      "transformer.h.2.ln_cross_attn.weight\n",
      "transformer.h.2.ln_cross_attn.bias\n",
      "transformer.h.2.mlp.c_fc.weight\n",
      "transformer.h.2.mlp.c_fc.bias\n",
      "transformer.h.2.mlp.c_proj.weight\n",
      "transformer.h.2.mlp.c_proj.bias\n",
      "transformer.h.3.ln_1.weight\n",
      "transformer.h.3.ln_1.bias\n",
      "transformer.h.3.attn.c_attn.weight\n",
      "transformer.h.3.attn.c_attn.bias\n",
      "transformer.h.3.attn.c_proj.weight\n",
      "transformer.h.3.attn.c_proj.bias\n",
      "transformer.h.3.ln_2.weight\n",
      "transformer.h.3.ln_2.bias\n",
      "transformer.h.3.crossattention.c_attn.weight\n",
      "transformer.h.3.crossattention.c_attn.bias\n",
      "transformer.h.3.crossattention.q_attn.weight\n",
      "transformer.h.3.crossattention.q_attn.bias\n",
      "transformer.h.3.crossattention.c_proj.weight\n",
      "transformer.h.3.crossattention.c_proj.bias\n",
      "transformer.h.3.ln_cross_attn.weight\n",
      "transformer.h.3.ln_cross_attn.bias\n",
      "transformer.h.3.mlp.c_fc.weight\n",
      "transformer.h.3.mlp.c_fc.bias\n",
      "transformer.h.3.mlp.c_proj.weight\n",
      "transformer.h.3.mlp.c_proj.bias\n",
      "transformer.h.4.ln_1.weight\n",
      "transformer.h.4.ln_1.bias\n",
      "transformer.h.4.attn.c_attn.weight\n",
      "transformer.h.4.attn.c_attn.bias\n",
      "transformer.h.4.attn.c_proj.weight\n",
      "transformer.h.4.attn.c_proj.bias\n",
      "transformer.h.4.ln_2.weight\n",
      "transformer.h.4.ln_2.bias\n",
      "transformer.h.4.crossattention.c_attn.weight\n",
      "transformer.h.4.crossattention.c_attn.bias\n",
      "transformer.h.4.crossattention.q_attn.weight\n",
      "transformer.h.4.crossattention.q_attn.bias\n",
      "transformer.h.4.crossattention.c_proj.weight\n",
      "transformer.h.4.crossattention.c_proj.bias\n",
      "transformer.h.4.ln_cross_attn.weight\n",
      "transformer.h.4.ln_cross_attn.bias\n",
      "transformer.h.4.mlp.c_fc.weight\n",
      "transformer.h.4.mlp.c_fc.bias\n",
      "transformer.h.4.mlp.c_proj.weight\n",
      "transformer.h.4.mlp.c_proj.bias\n",
      "transformer.h.5.ln_1.weight\n",
      "transformer.h.5.ln_1.bias\n",
      "transformer.h.5.attn.c_attn.weight\n",
      "transformer.h.5.attn.c_attn.bias\n",
      "transformer.h.5.attn.c_proj.weight\n",
      "transformer.h.5.attn.c_proj.bias\n",
      "transformer.h.5.ln_2.weight\n",
      "transformer.h.5.ln_2.bias\n",
      "transformer.h.5.crossattention.c_attn.weight\n",
      "transformer.h.5.crossattention.c_attn.bias\n",
      "transformer.h.5.crossattention.q_attn.weight\n",
      "transformer.h.5.crossattention.q_attn.bias\n",
      "transformer.h.5.crossattention.c_proj.weight\n",
      "transformer.h.5.crossattention.c_proj.bias\n",
      "transformer.h.5.ln_cross_attn.weight\n",
      "transformer.h.5.ln_cross_attn.bias\n",
      "transformer.h.5.mlp.c_fc.weight\n",
      "transformer.h.5.mlp.c_fc.bias\n",
      "transformer.h.5.mlp.c_proj.weight\n",
      "transformer.h.5.mlp.c_proj.bias\n",
      "transformer.ln_f.weight\n",
      "transformer.ln_f.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.decoder.named_parameters():\n",
    "    print(name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Freeze layers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7b61610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze layer of the encoder (Assuming that the model is already good at understanding images). Since all cross-attention weights need to be optimized\n",
    "# I do not freeze any of the decoder layers.\n",
    "for name, param in model.encoder.named_parameters():\n",
    "    # freeze stage 1 and 2 of the Swin encoder.\n",
    "    if 'encoder.layer.3' in name:\n",
    "        break\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation Metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"rouge\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ignore_pad_token_for_loss = True\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "\n",
    "\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"\n",
    "     Process the predicted captions and the labels (image captions) to use the feed them to the Rough Metric to compute evaluation metrics.\n",
    "    :param eval_preds: set of predicted and target tokens\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    #predicted captions are decoded into strings using GPT-2 tokenizer\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    if ignore_pad_token_for_loss:\n",
    "        # the token ID -100 indicates the end of the sequence.\n",
    "        # Replaces all -100 values with the id of the padding token in the tokeniezer\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # decoded strings are furthered pre-preprocessed e.g., split them into sentences using `sentence_tokenize` from NLTK\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds,\n",
    "                                                     decoded_labels)\n",
    "    # Compute the Rough metric. The metric uses stemming to match words with the same root.\n",
    "    result = metric.compute(predictions=decoded_preds,\n",
    "                            references=decoded_labels,\n",
    "                            use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
    "    ]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Procedure:\n",
    "\n",
    "- The swin-gpt-2-image-captioning model is trained with the `Trainer` class from the transformers library.\n",
    "- The arguments used for training are set via `TrainingArguments` class.\n",
    "- The training is started by the `train ` method of the trainer class."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6825c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arg = TrainingArguments(\n",
    "    output_dir='../models/swin_image_captioning', # The output directory\n",
    "    overwrite_output_dir=True, # overwrite the content of the output directory\n",
    "    num_train_epochs=2, # number of training epochs\n",
    "    per_device_train_batch_size=64, # batch size for training\n",
    "    per_device_eval_batch_size=64,  # batch size for evaluation\n",
    "    load_best_model_at_end=True,\n",
    "    log_level='info',\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arg,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=processed_dataset['train'],\n",
    "    eval_dataset=processed_dataset['test'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2eb998fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `VisionEncoderDecoderModel.forward` and have been ignored: caption_id, coco_url, file_name, height, image_id, width. If caption_id, coco_url, file_name, height, image_id, width are not expected by `VisionEncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 99\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/2 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': 'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/transformers/trainer.py:2774\u001B[0m, in \u001B[0;36mTrainer.evaluate\u001B[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001B[0m\n\u001B[1;32m   2771\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m   2773\u001B[0m eval_loop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprediction_loop \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39muse_legacy_prediction_loop \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluation_loop\n\u001B[0;32m-> 2774\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43meval_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2775\u001B[0m \u001B[43m    \u001B[49m\u001B[43meval_dataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2776\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdescription\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mEvaluation\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2777\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001B[39;49;00m\n\u001B[1;32m   2778\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# self.args.prediction_loss_only\u001B[39;49;00m\n\u001B[1;32m   2779\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprediction_loss_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_metrics\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   2780\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2781\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmetric_key_prefix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetric_key_prefix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2782\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2784\u001B[0m total_batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39meval_batch_size \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mworld_size\n\u001B[1;32m   2785\u001B[0m output\u001B[38;5;241m.\u001B[39mmetrics\u001B[38;5;241m.\u001B[39mupdate(\n\u001B[1;32m   2786\u001B[0m     speed_metrics(\n\u001B[1;32m   2787\u001B[0m         metric_key_prefix,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2791\u001B[0m     )\n\u001B[1;32m   2792\u001B[0m )\n",
      "File \u001B[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/transformers/trainer.py:3059\u001B[0m, in \u001B[0;36mTrainer.evaluation_loop\u001B[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001B[0m\n\u001B[1;32m   3055\u001B[0m         metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_metrics(\n\u001B[1;32m   3056\u001B[0m             EvalPrediction(predictions\u001B[38;5;241m=\u001B[39mall_preds, label_ids\u001B[38;5;241m=\u001B[39mall_labels, inputs\u001B[38;5;241m=\u001B[39mall_inputs)\n\u001B[1;32m   3057\u001B[0m         )\n\u001B[1;32m   3058\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 3059\u001B[0m         metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_metrics\u001B[49m\u001B[43m(\u001B[49m\u001B[43mEvalPrediction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpredictions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mall_preds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mall_labels\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3060\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   3061\u001B[0m     metrics \u001B[38;5;241m=\u001B[39m {}\n",
      "Cell \u001B[0;32mIn[18], line 30\u001B[0m, in \u001B[0;36mcompute_metrics\u001B[0;34m(eval_preds)\u001B[0m\n\u001B[1;32m     27\u001B[0m     preds \u001B[38;5;241m=\u001B[39m preds[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     29\u001B[0m \u001B[38;5;66;03m#predicted captions are decoded into strings using GPT-2 tokenizer\u001B[39;00m\n\u001B[0;32m---> 30\u001B[0m decoded_preds \u001B[38;5;241m=\u001B[39m \u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_decode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpreds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskip_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ignore_pad_token_for_loss:\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;66;03m# the token ID -100 indicates the end of the sequence.\u001B[39;00m\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;66;03m# Replaces all -100 values with the id of the padding token in the tokeniezer\u001B[39;00m\n\u001B[1;32m     34\u001B[0m     labels \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mwhere(labels \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m100\u001B[39m, labels, tokenizer\u001B[38;5;241m.\u001B[39mpad_token_id)\n",
      "File \u001B[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3393\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.batch_decode\u001B[0;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001B[0m\n\u001B[1;32m   3370\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbatch_decode\u001B[39m(\n\u001B[1;32m   3371\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   3372\u001B[0m     sequences: Union[List[\u001B[38;5;28mint\u001B[39m], List[List[\u001B[38;5;28mint\u001B[39m]], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnp.ndarray\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch.Tensor\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf.Tensor\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3375\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m   3376\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m   3377\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3378\u001B[0m \u001B[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001B[39;00m\n\u001B[1;32m   3379\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3391\u001B[0m \u001B[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001B[39;00m\n\u001B[1;32m   3392\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3393\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[1;32m   3394\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecode(\n\u001B[1;32m   3395\u001B[0m             seq,\n\u001B[1;32m   3396\u001B[0m             skip_special_tokens\u001B[38;5;241m=\u001B[39mskip_special_tokens,\n\u001B[1;32m   3397\u001B[0m             clean_up_tokenization_spaces\u001B[38;5;241m=\u001B[39mclean_up_tokenization_spaces,\n\u001B[1;32m   3398\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   3399\u001B[0m         )\n\u001B[1;32m   3400\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m seq \u001B[38;5;129;01min\u001B[39;00m sequences\n\u001B[1;32m   3401\u001B[0m     ]\n",
      "File \u001B[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3394\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   3370\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbatch_decode\u001B[39m(\n\u001B[1;32m   3371\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   3372\u001B[0m     sequences: Union[List[\u001B[38;5;28mint\u001B[39m], List[List[\u001B[38;5;28mint\u001B[39m]], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnp.ndarray\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch.Tensor\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf.Tensor\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3375\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m   3376\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m   3377\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3378\u001B[0m \u001B[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001B[39;00m\n\u001B[1;32m   3379\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3391\u001B[0m \u001B[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001B[39;00m\n\u001B[1;32m   3392\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m   3393\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[0;32m-> 3394\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3395\u001B[0m \u001B[43m            \u001B[49m\u001B[43mseq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3396\u001B[0m \u001B[43m            \u001B[49m\u001B[43mskip_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskip_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3397\u001B[0m \u001B[43m            \u001B[49m\u001B[43mclean_up_tokenization_spaces\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclean_up_tokenization_spaces\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3398\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3399\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3400\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m seq \u001B[38;5;129;01min\u001B[39;00m sequences\n\u001B[1;32m   3401\u001B[0m     ]\n",
      "File \u001B[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3432\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.decode\u001B[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001B[0m\n\u001B[1;32m   3429\u001B[0m \u001B[38;5;66;03m# Convert inputs to python lists\u001B[39;00m\n\u001B[1;32m   3430\u001B[0m token_ids \u001B[38;5;241m=\u001B[39m to_py_obj(token_ids)\n\u001B[0;32m-> 3432\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_decode\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3433\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3434\u001B[0m \u001B[43m    \u001B[49m\u001B[43mskip_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskip_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3435\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclean_up_tokenization_spaces\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclean_up_tokenization_spaces\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3436\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3437\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:549\u001B[0m, in \u001B[0;36mPreTrainedTokenizerFast._decode\u001B[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001B[0m\n\u001B[1;32m    547\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(token_ids, \u001B[38;5;28mint\u001B[39m):\n\u001B[1;32m    548\u001B[0m     token_ids \u001B[38;5;241m=\u001B[39m [token_ids]\n\u001B[0;32m--> 549\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtoken_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskip_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskip_special_tokens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    551\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m clean_up_tokenization_spaces:\n\u001B[1;32m    552\u001B[0m     clean_text \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclean_up_tokenization(text)\n",
      "\u001B[0;31mTypeError\u001B[0m: argument 'ids': 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eed88a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4 epochs took 8 hours\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c1ef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#need to save the tokenizer\n",
    "tokenizer.save_pretrained('../models/swin_image_captioning')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc3b39e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# loading model and config from pretrained folder\n",
    "finetuned_model = VisionEncoderDecoderModel.from_pretrained('../models/swin_image_captioning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from IPython.core.display_functions import display\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inference_transforms = transforms.Compose(\n",
    "    [\n",
    "\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# a helper function to caption images from the web or a file path\n",
    "def generate_caption(m, path):\n",
    "\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    img_transformed = inference_transforms(img).unsqueeze(0)\n",
    "\n",
    "    model_output = m.generate(\n",
    "        img_transformed,\n",
    "        num_beams=3,\n",
    "        max_length=15,\n",
    "        early_stopping=True,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=5,\n",
    "    )\n",
    "\n",
    "    captions = [tokenizer.decode(g, skip_special_tokens=True).strip() for g in model_output]\n",
    "    #Show image\n",
    "    display(img)\n",
    "    return captions, model_output, img_transformed\n",
    "\n",
    "\n",
    "captions, model_output, img_transformed = generate_caption(  # Out of sample photo\n",
    "    finetuned_model, '../data/test_data/000000421195_test.jpg'\n",
    ")\n",
    "\n",
    "captions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#from transformers import pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# image_captioner = pipeline(\"image-to-text\", model=\"../models/swin_image_captioning\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}