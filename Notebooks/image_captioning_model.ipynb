{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7226c3a",
   "metadata": {},
   "source": [
    "## Image-captioning model\n",
    "\n",
    "### Overview\n",
    "\n",
    "The code below trains an image captioning algorithm using a pre-trained vision and language transformer as well as the COCO dataset. The model uses the Microsoft Swin-Tiny-Patch4-Window7-224 and DistilGPT-2  pre-trained models as the encoder and decoder respectively.\n",
    "The code implementation is done with the help of the following libraries/packages:\n",
    " - Transformers library from Hugging Face to implement the model.\n",
    " - PyTorch for tensor operations\n",
    " - PIL and torchvision libraries for image processing.\n",
    "\n",
    "\n",
    "### Instructions to Run this notebook\n",
    "\n",
    "To run this code on a local machine, follow these steps:\n",
    "\n",
    "- Install the requirements listed in `requirements.txt`. Make sure to include the Transformers, datasets, scikit-learn, PIL, and PyTorch libraries/packages\n",
    "\n",
    "- Download the COCO dataset and place it in the directory specified by the COCO_DIR variable.\n",
    "\n",
    "Run the code in a Python environment that has access to a GPU. The code can be run in a Jupyter notebook or in a Python script. This code was run on MacBook M1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "41994dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from transformers import VisionEncoderDecoderModel, GPT2TokenizerFast, AutoFeatureExtractor, \\\n",
    "                          TrainingArguments, Trainer\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4167cac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yesidcano/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1673856676759/work/aten/src/ATen/native/TensorShape.cpp:3454.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Some weights of the model checkpoint at microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['transformer.h.3.crossattention.masked_bias', 'transformer.h.3.crossattention.bias', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.4.crossattention.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.2.crossattention.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.5.crossattention.bias', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.1.crossattention.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.1.crossattention.masked_bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.2.crossattention.masked_bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.4.crossattention.masked_bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.0.crossattention.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.5.crossattention.masked_bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.0.crossattention.masked_bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.4.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model uses a rre-trained encoder of type <class 'transformers.models.swin.modeling_swin.SwinModel'> and pre-trained decoder of type <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>\n"
     ]
    }
   ],
   "source": [
    "# ???Many weights are initialized randomly, namely the cross attention weights??? change this comment. not mine\n",
    "#One of the main objectives is optimizing the cross-attention weights. How GPT connects to the encoders output,\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    'microsoft/swin-tiny-patch4-window7-224',\n",
    "    'distilgpt2'\n",
    ")\n",
    "\n",
    "print(f'This model uses a rre-trained encoder of type {type(model.encoder)} and pre-trained decoder of type {type(model.decoder)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Device\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "RadioButtons(description='Select device', options=('M1', 'Other'), value='M1')",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2a1a6f223824f2ebbe9502ffca822e2"
      }
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_type = widgets.RadioButtons(\n",
    "    options=['M1', 'Other'],\n",
    "    value='M1',\n",
    "    description='Select device',\n",
    "    disabled=False\n",
    ")\n",
    "device_type"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "VisionEncoderDecoderModel(\n  (encoder): SwinModel(\n    (embeddings): SwinEmbeddings(\n      (patch_embeddings): SwinPatchEmbeddings(\n        (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n      )\n      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): SwinEncoder(\n      (layers): ModuleList(\n        (0): SwinStage(\n          (blocks): ModuleList(\n            (0-1): 2 x SwinLayer(\n              (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=96, out_features=96, bias=True)\n                  (key): Linear(in_features=96, out_features=96, bias=True)\n                  (value): Linear(in_features=96, out_features=96, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=96, out_features=96, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.1)\n              (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=96, out_features=384, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=384, out_features=96, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n          )\n          (downsample): SwinPatchMerging(\n            (reduction): Linear(in_features=384, out_features=192, bias=False)\n            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (1): SwinStage(\n          (blocks): ModuleList(\n            (0-1): 2 x SwinLayer(\n              (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=192, out_features=192, bias=True)\n                  (key): Linear(in_features=192, out_features=192, bias=True)\n                  (value): Linear(in_features=192, out_features=192, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=192, out_features=192, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.1)\n              (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=192, out_features=768, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=768, out_features=192, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n          )\n          (downsample): SwinPatchMerging(\n            (reduction): Linear(in_features=768, out_features=384, bias=False)\n            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (2): SwinStage(\n          (blocks): ModuleList(\n            (0-5): 6 x SwinLayer(\n              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=384, out_features=384, bias=True)\n                  (key): Linear(in_features=384, out_features=384, bias=True)\n                  (value): Linear(in_features=384, out_features=384, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=384, out_features=384, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.1)\n              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=384, out_features=1536, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=1536, out_features=384, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n          )\n          (downsample): SwinPatchMerging(\n            (reduction): Linear(in_features=1536, out_features=768, bias=False)\n            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (3): SwinStage(\n          (blocks): ModuleList(\n            (0-1): 2 x SwinLayer(\n              (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (attention): SwinAttention(\n                (self): SwinSelfAttention(\n                  (query): Linear(in_features=768, out_features=768, bias=True)\n                  (key): Linear(in_features=768, out_features=768, bias=True)\n                  (value): Linear(in_features=768, out_features=768, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n                (output): SwinSelfOutput(\n                  (dense): Linear(in_features=768, out_features=768, bias=True)\n                  (dropout): Dropout(p=0.0, inplace=False)\n                )\n              )\n              (drop_path): SwinDropPath(p=0.1)\n              (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (intermediate): SwinIntermediate(\n                (dense): Linear(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): SwinOutput(\n                (dense): Linear(in_features=3072, out_features=768, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n          )\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (pooler): AdaptiveAvgPool1d(output_size=1)\n  )\n  (decoder): GPT2LMHeadModel(\n    (transformer): GPT2Model(\n      (wte): Embedding(50257, 768)\n      (wpe): Embedding(1024, 768)\n      (drop): Dropout(p=0.1, inplace=False)\n      (h): ModuleList(\n        (0-5): 6 x GPT2Block(\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): GPT2Attention(\n            (c_attn): Conv1D()\n            (c_proj): Conv1D()\n            (attn_dropout): Dropout(p=0.1, inplace=False)\n            (resid_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (crossattention): GPT2Attention(\n            (c_attn): Conv1D()\n            (q_attn): Conv1D()\n            (c_proj): Conv1D()\n            (attn_dropout): Dropout(p=0.1, inplace=False)\n            (resid_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): GPT2MLP(\n            (c_fc): Conv1D()\n            (c_proj): Conv1D()\n            (act): NewGELUActivation()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n  )\n)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train on gpu\n",
    "if device_type.value == 'M1':\n",
    "\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4e9f1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 123,615,354\n"
     ]
    }
   ],
   "source": [
    "# Vit and GPT2 have 182,485,248 combined parameters. Swin version I am tuning here has fewer parameters\n",
    "from torch import numel\n",
    "\n",
    "combined_params = 0\n",
    "for param in model.parameters():\n",
    "    combined_params += numel(param)\n",
    "    \n",
    "print(f\"Total number of parameters: {combined_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Database\n",
    "The image captioning algorithm is trained on the COCO dataset which consists of images and captions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# COCO_DIR = input('Path to COCO dataset')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset coco_dataset_script (/Users/yesidcano/.cache/huggingface/datasets/ydshieh___coco_dataset_script/2017-6b5176efb5303df4/0.0.0/e033205c0266a54c10be132f9264f2a39dcf893e798f6756d224b1ff5078998f)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['image_id', 'caption_id', 'caption', 'height', 'width', 'file_name', 'coco_url', 'image_path'],\n    num_rows: 60\n})"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the datasets.load_dataset manages everything related to caching. So I have to use it.\n",
    "COCO_DIR = '/Users/yesidcano/repos/image-captioning/data/coco'\n",
    "\n",
    "\n",
    "#ds = datasets.load_dataset(\"ydshieh/coco_dataset_script\", \"2017\",data_dir=COCO_DIR, cache_dir='/Users/yesidcano/repos/db_coco_cache')\n",
    "\n",
    "# Load a slice of the database this https://huggingface.co/docs/datasets/loading to split dataset.\n",
    "\n",
    "ds = datasets.load_dataset(\"ydshieh/coco_dataset_script\", \"2017\",data_dir=COCO_DIR, split=\"train[10:70]\")\n",
    "ds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a4450a9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file preprocessor_config.json from cache at /Users/yesidcano/.cache/huggingface/hub/models--microsoft--swin-tiny-patch4-window7-224/snapshots/83d40fb5b9320b349382208d9e7fe998484e99df/preprocessor_config.json\n",
      "Feature extractor ViTFeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.485,\n",
      "    0.456,\n",
      "    0.406\n",
      "  ],\n",
      "  \"image_std\": [\n",
      "    0.229,\n",
      "    0.224,\n",
      "    0.225\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"size\": 224\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /Users/yesidcano/.cache/huggingface/hub/models--distilgpt2/snapshots/f241065e938b44ac52db2c5de82c8bd2fafc76d0/vocab.json\n",
      "loading file merges.txt from cache at /Users/yesidcano/.cache/huggingface/hub/models--distilgpt2/snapshots/f241065e938b44ac52db2c5de82c8bd2fafc76d0/merges.txt\n",
      "loading file tokenizer.json from cache at /Users/yesidcano/.cache/huggingface/hub/models--distilgpt2/snapshots/f241065e938b44ac52db2c5de82c8bd2fafc76d0/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /Users/yesidcano/.cache/huggingface/hub/models--distilgpt2/snapshots/f241065e938b44ac52db2c5de82c8bd2fafc76d0/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"distilgpt2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 6,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
    "#gpt-2 tokenizer\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('distilgpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Model config\n",
    "\n",
    "model.config.pad_token = tokenizer.pad_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "model.config.decoder_start_token = tokenizer.bos_token\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "ViTFeatureExtractor {\n  \"do_normalize\": true,\n  \"do_resize\": true,\n  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n  \"image_mean\": [\n    0.485,\n    0.456,\n    0.406\n  ],\n  \"image_std\": [\n    0.229,\n    0.224,\n    0.225\n  ],\n  \"resample\": 3,\n  \"size\": 224\n}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uses fast tokenizer\n",
    "tokenizer.is_fast"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/yesidcano/.cache/huggingface/datasets/ydshieh___coco_dataset_script/2017-6b5176efb5303df4/0.0.0/e033205c0266a54c10be132f9264f2a39dcf893e798f6756d224b1ff5078998f/cache-a92f64da606230b3.arrow\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Define the transforms to be applied to the images\n",
    "transform = transforms.Compose([\n",
    "\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "])\n",
    "\n",
    "def preprocess_fn(examples):\n",
    "\n",
    "    # Swin expects pixel_values instead of input_ids\n",
    "    examples['pixel_values'] = [transform(Image.open(path).convert('RGB')) for path in examples['image_path']]\n",
    "    # We are padding tokens here instead of using a datacollator\n",
    "    tokenized = tokenizer(\n",
    "        examples['caption'], padding='max_length', max_length=10, truncation=True\n",
    "    )['input_ids']\n",
    "    # the output captions\n",
    "    examples['labels'] = [[l if l != tokenizer.pad_token_id else -100 for l in t] for t in tokenized]\n",
    "\n",
    "    # delete unused keys\n",
    "    del examples['image_path']\n",
    "    del examples['caption']\n",
    "    return examples\n",
    "\n",
    "processed_dataset = ds.map(\n",
    "    function=preprocess_fn,\n",
    "    batched=True,\n",
    "    batch_size = 50,\n",
    "    #remove_columns=['image_id', 'caption_id', 'caption', 'height', 'width', 'file_name', 'coco_url', 'image_path']\n",
    "\n",
    "\n",
    "\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5ca503c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['image_id', 'caption_id', 'height', 'width', 'file_name', 'coco_url', 'pixel_values', 'labels'],\n        num_rows: 54\n    })\n    test: Dataset({\n        features: ['image_id', 'caption_id', 'height', 'width', 'file_name', 'coco_url', 'pixel_values', 'labels'],\n        num_rows: 6\n    })\n})"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By default data are shuffled.\n",
    "processed_dataset = processed_dataset.train_test_split(test_size=0.1)\n",
    "processed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model's layers\n",
    "Below a list of the encoder and decoder's layers respectively."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Encoder Layers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.patch_embeddings.projection.weight\n",
      "embeddings.patch_embeddings.projection.bias\n",
      "embeddings.norm.weight\n",
      "embeddings.norm.bias\n",
      "encoder.layers.0.blocks.0.layernorm_before.weight\n",
      "encoder.layers.0.blocks.0.layernorm_before.bias\n",
      "encoder.layers.0.blocks.0.attention.self.relative_position_bias_table\n",
      "encoder.layers.0.blocks.0.attention.self.query.weight\n",
      "encoder.layers.0.blocks.0.attention.self.query.bias\n",
      "encoder.layers.0.blocks.0.attention.self.key.weight\n",
      "encoder.layers.0.blocks.0.attention.self.key.bias\n",
      "encoder.layers.0.blocks.0.attention.self.value.weight\n",
      "encoder.layers.0.blocks.0.attention.self.value.bias\n",
      "encoder.layers.0.blocks.0.attention.output.dense.weight\n",
      "encoder.layers.0.blocks.0.attention.output.dense.bias\n",
      "encoder.layers.0.blocks.0.layernorm_after.weight\n",
      "encoder.layers.0.blocks.0.layernorm_after.bias\n",
      "encoder.layers.0.blocks.0.intermediate.dense.weight\n",
      "encoder.layers.0.blocks.0.intermediate.dense.bias\n",
      "encoder.layers.0.blocks.0.output.dense.weight\n",
      "encoder.layers.0.blocks.0.output.dense.bias\n",
      "encoder.layers.0.blocks.1.layernorm_before.weight\n",
      "encoder.layers.0.blocks.1.layernorm_before.bias\n",
      "encoder.layers.0.blocks.1.attention.self.relative_position_bias_table\n",
      "encoder.layers.0.blocks.1.attention.self.query.weight\n",
      "encoder.layers.0.blocks.1.attention.self.query.bias\n",
      "encoder.layers.0.blocks.1.attention.self.key.weight\n",
      "encoder.layers.0.blocks.1.attention.self.key.bias\n",
      "encoder.layers.0.blocks.1.attention.self.value.weight\n",
      "encoder.layers.0.blocks.1.attention.self.value.bias\n",
      "encoder.layers.0.blocks.1.attention.output.dense.weight\n",
      "encoder.layers.0.blocks.1.attention.output.dense.bias\n",
      "encoder.layers.0.blocks.1.layernorm_after.weight\n",
      "encoder.layers.0.blocks.1.layernorm_after.bias\n",
      "encoder.layers.0.blocks.1.intermediate.dense.weight\n",
      "encoder.layers.0.blocks.1.intermediate.dense.bias\n",
      "encoder.layers.0.blocks.1.output.dense.weight\n",
      "encoder.layers.0.blocks.1.output.dense.bias\n",
      "encoder.layers.0.downsample.reduction.weight\n",
      "encoder.layers.0.downsample.norm.weight\n",
      "encoder.layers.0.downsample.norm.bias\n",
      "encoder.layers.1.blocks.0.layernorm_before.weight\n",
      "encoder.layers.1.blocks.0.layernorm_before.bias\n",
      "encoder.layers.1.blocks.0.attention.self.relative_position_bias_table\n",
      "encoder.layers.1.blocks.0.attention.self.query.weight\n",
      "encoder.layers.1.blocks.0.attention.self.query.bias\n",
      "encoder.layers.1.blocks.0.attention.self.key.weight\n",
      "encoder.layers.1.blocks.0.attention.self.key.bias\n",
      "encoder.layers.1.blocks.0.attention.self.value.weight\n",
      "encoder.layers.1.blocks.0.attention.self.value.bias\n",
      "encoder.layers.1.blocks.0.attention.output.dense.weight\n",
      "encoder.layers.1.blocks.0.attention.output.dense.bias\n",
      "encoder.layers.1.blocks.0.layernorm_after.weight\n",
      "encoder.layers.1.blocks.0.layernorm_after.bias\n",
      "encoder.layers.1.blocks.0.intermediate.dense.weight\n",
      "encoder.layers.1.blocks.0.intermediate.dense.bias\n",
      "encoder.layers.1.blocks.0.output.dense.weight\n",
      "encoder.layers.1.blocks.0.output.dense.bias\n",
      "encoder.layers.1.blocks.1.layernorm_before.weight\n",
      "encoder.layers.1.blocks.1.layernorm_before.bias\n",
      "encoder.layers.1.blocks.1.attention.self.relative_position_bias_table\n",
      "encoder.layers.1.blocks.1.attention.self.query.weight\n",
      "encoder.layers.1.blocks.1.attention.self.query.bias\n",
      "encoder.layers.1.blocks.1.attention.self.key.weight\n",
      "encoder.layers.1.blocks.1.attention.self.key.bias\n",
      "encoder.layers.1.blocks.1.attention.self.value.weight\n",
      "encoder.layers.1.blocks.1.attention.self.value.bias\n",
      "encoder.layers.1.blocks.1.attention.output.dense.weight\n",
      "encoder.layers.1.blocks.1.attention.output.dense.bias\n",
      "encoder.layers.1.blocks.1.layernorm_after.weight\n",
      "encoder.layers.1.blocks.1.layernorm_after.bias\n",
      "encoder.layers.1.blocks.1.intermediate.dense.weight\n",
      "encoder.layers.1.blocks.1.intermediate.dense.bias\n",
      "encoder.layers.1.blocks.1.output.dense.weight\n",
      "encoder.layers.1.blocks.1.output.dense.bias\n",
      "encoder.layers.1.downsample.reduction.weight\n",
      "encoder.layers.1.downsample.norm.weight\n",
      "encoder.layers.1.downsample.norm.bias\n",
      "encoder.layers.2.blocks.0.layernorm_before.weight\n",
      "encoder.layers.2.blocks.0.layernorm_before.bias\n",
      "encoder.layers.2.blocks.0.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.0.attention.self.query.weight\n",
      "encoder.layers.2.blocks.0.attention.self.query.bias\n",
      "encoder.layers.2.blocks.0.attention.self.key.weight\n",
      "encoder.layers.2.blocks.0.attention.self.key.bias\n",
      "encoder.layers.2.blocks.0.attention.self.value.weight\n",
      "encoder.layers.2.blocks.0.attention.self.value.bias\n",
      "encoder.layers.2.blocks.0.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.0.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.0.layernorm_after.weight\n",
      "encoder.layers.2.blocks.0.layernorm_after.bias\n",
      "encoder.layers.2.blocks.0.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.0.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.0.output.dense.weight\n",
      "encoder.layers.2.blocks.0.output.dense.bias\n",
      "encoder.layers.2.blocks.1.layernorm_before.weight\n",
      "encoder.layers.2.blocks.1.layernorm_before.bias\n",
      "encoder.layers.2.blocks.1.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.1.attention.self.query.weight\n",
      "encoder.layers.2.blocks.1.attention.self.query.bias\n",
      "encoder.layers.2.blocks.1.attention.self.key.weight\n",
      "encoder.layers.2.blocks.1.attention.self.key.bias\n",
      "encoder.layers.2.blocks.1.attention.self.value.weight\n",
      "encoder.layers.2.blocks.1.attention.self.value.bias\n",
      "encoder.layers.2.blocks.1.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.1.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.1.layernorm_after.weight\n",
      "encoder.layers.2.blocks.1.layernorm_after.bias\n",
      "encoder.layers.2.blocks.1.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.1.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.1.output.dense.weight\n",
      "encoder.layers.2.blocks.1.output.dense.bias\n",
      "encoder.layers.2.blocks.2.layernorm_before.weight\n",
      "encoder.layers.2.blocks.2.layernorm_before.bias\n",
      "encoder.layers.2.blocks.2.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.2.attention.self.query.weight\n",
      "encoder.layers.2.blocks.2.attention.self.query.bias\n",
      "encoder.layers.2.blocks.2.attention.self.key.weight\n",
      "encoder.layers.2.blocks.2.attention.self.key.bias\n",
      "encoder.layers.2.blocks.2.attention.self.value.weight\n",
      "encoder.layers.2.blocks.2.attention.self.value.bias\n",
      "encoder.layers.2.blocks.2.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.2.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.2.layernorm_after.weight\n",
      "encoder.layers.2.blocks.2.layernorm_after.bias\n",
      "encoder.layers.2.blocks.2.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.2.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.2.output.dense.weight\n",
      "encoder.layers.2.blocks.2.output.dense.bias\n",
      "encoder.layers.2.blocks.3.layernorm_before.weight\n",
      "encoder.layers.2.blocks.3.layernorm_before.bias\n",
      "encoder.layers.2.blocks.3.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.3.attention.self.query.weight\n",
      "encoder.layers.2.blocks.3.attention.self.query.bias\n",
      "encoder.layers.2.blocks.3.attention.self.key.weight\n",
      "encoder.layers.2.blocks.3.attention.self.key.bias\n",
      "encoder.layers.2.blocks.3.attention.self.value.weight\n",
      "encoder.layers.2.blocks.3.attention.self.value.bias\n",
      "encoder.layers.2.blocks.3.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.3.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.3.layernorm_after.weight\n",
      "encoder.layers.2.blocks.3.layernorm_after.bias\n",
      "encoder.layers.2.blocks.3.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.3.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.3.output.dense.weight\n",
      "encoder.layers.2.blocks.3.output.dense.bias\n",
      "encoder.layers.2.blocks.4.layernorm_before.weight\n",
      "encoder.layers.2.blocks.4.layernorm_before.bias\n",
      "encoder.layers.2.blocks.4.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.4.attention.self.query.weight\n",
      "encoder.layers.2.blocks.4.attention.self.query.bias\n",
      "encoder.layers.2.blocks.4.attention.self.key.weight\n",
      "encoder.layers.2.blocks.4.attention.self.key.bias\n",
      "encoder.layers.2.blocks.4.attention.self.value.weight\n",
      "encoder.layers.2.blocks.4.attention.self.value.bias\n",
      "encoder.layers.2.blocks.4.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.4.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.4.layernorm_after.weight\n",
      "encoder.layers.2.blocks.4.layernorm_after.bias\n",
      "encoder.layers.2.blocks.4.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.4.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.4.output.dense.weight\n",
      "encoder.layers.2.blocks.4.output.dense.bias\n",
      "encoder.layers.2.blocks.5.layernorm_before.weight\n",
      "encoder.layers.2.blocks.5.layernorm_before.bias\n",
      "encoder.layers.2.blocks.5.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.5.attention.self.query.weight\n",
      "encoder.layers.2.blocks.5.attention.self.query.bias\n",
      "encoder.layers.2.blocks.5.attention.self.key.weight\n",
      "encoder.layers.2.blocks.5.attention.self.key.bias\n",
      "encoder.layers.2.blocks.5.attention.self.value.weight\n",
      "encoder.layers.2.blocks.5.attention.self.value.bias\n",
      "encoder.layers.2.blocks.5.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.5.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.5.layernorm_after.weight\n",
      "encoder.layers.2.blocks.5.layernorm_after.bias\n",
      "encoder.layers.2.blocks.5.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.5.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.5.output.dense.weight\n",
      "encoder.layers.2.blocks.5.output.dense.bias\n",
      "encoder.layers.2.downsample.reduction.weight\n",
      "encoder.layers.2.downsample.norm.weight\n",
      "encoder.layers.2.downsample.norm.bias\n",
      "encoder.layers.3.blocks.0.layernorm_before.weight\n",
      "encoder.layers.3.blocks.0.layernorm_before.bias\n",
      "encoder.layers.3.blocks.0.attention.self.relative_position_bias_table\n",
      "encoder.layers.3.blocks.0.attention.self.query.weight\n",
      "encoder.layers.3.blocks.0.attention.self.query.bias\n",
      "encoder.layers.3.blocks.0.attention.self.key.weight\n",
      "encoder.layers.3.blocks.0.attention.self.key.bias\n",
      "encoder.layers.3.blocks.0.attention.self.value.weight\n",
      "encoder.layers.3.blocks.0.attention.self.value.bias\n",
      "encoder.layers.3.blocks.0.attention.output.dense.weight\n",
      "encoder.layers.3.blocks.0.attention.output.dense.bias\n",
      "encoder.layers.3.blocks.0.layernorm_after.weight\n",
      "encoder.layers.3.blocks.0.layernorm_after.bias\n",
      "encoder.layers.3.blocks.0.intermediate.dense.weight\n",
      "encoder.layers.3.blocks.0.intermediate.dense.bias\n",
      "encoder.layers.3.blocks.0.output.dense.weight\n",
      "encoder.layers.3.blocks.0.output.dense.bias\n",
      "encoder.layers.3.blocks.1.layernorm_before.weight\n",
      "encoder.layers.3.blocks.1.layernorm_before.bias\n",
      "encoder.layers.3.blocks.1.attention.self.relative_position_bias_table\n",
      "encoder.layers.3.blocks.1.attention.self.query.weight\n",
      "encoder.layers.3.blocks.1.attention.self.query.bias\n",
      "encoder.layers.3.blocks.1.attention.self.key.weight\n",
      "encoder.layers.3.blocks.1.attention.self.key.bias\n",
      "encoder.layers.3.blocks.1.attention.self.value.weight\n",
      "encoder.layers.3.blocks.1.attention.self.value.bias\n",
      "encoder.layers.3.blocks.1.attention.output.dense.weight\n",
      "encoder.layers.3.blocks.1.attention.output.dense.bias\n",
      "encoder.layers.3.blocks.1.layernorm_after.weight\n",
      "encoder.layers.3.blocks.1.layernorm_after.bias\n",
      "encoder.layers.3.blocks.1.intermediate.dense.weight\n",
      "encoder.layers.3.blocks.1.intermediate.dense.bias\n",
      "encoder.layers.3.blocks.1.output.dense.weight\n",
      "encoder.layers.3.blocks.1.output.dense.bias\n",
      "layernorm.weight\n",
      "layernorm.bias\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for name, param in model.encoder.named_parameters():\n",
    "    print(name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Decoder's layers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight\n",
      "transformer.wpe.weight\n",
      "transformer.h.0.ln_1.weight\n",
      "transformer.h.0.ln_1.bias\n",
      "transformer.h.0.attn.c_attn.weight\n",
      "transformer.h.0.attn.c_attn.bias\n",
      "transformer.h.0.attn.c_proj.weight\n",
      "transformer.h.0.attn.c_proj.bias\n",
      "transformer.h.0.ln_2.weight\n",
      "transformer.h.0.ln_2.bias\n",
      "transformer.h.0.crossattention.c_attn.weight\n",
      "transformer.h.0.crossattention.c_attn.bias\n",
      "transformer.h.0.crossattention.q_attn.weight\n",
      "transformer.h.0.crossattention.q_attn.bias\n",
      "transformer.h.0.crossattention.c_proj.weight\n",
      "transformer.h.0.crossattention.c_proj.bias\n",
      "transformer.h.0.ln_cross_attn.weight\n",
      "transformer.h.0.ln_cross_attn.bias\n",
      "transformer.h.0.mlp.c_fc.weight\n",
      "transformer.h.0.mlp.c_fc.bias\n",
      "transformer.h.0.mlp.c_proj.weight\n",
      "transformer.h.0.mlp.c_proj.bias\n",
      "transformer.h.1.ln_1.weight\n",
      "transformer.h.1.ln_1.bias\n",
      "transformer.h.1.attn.c_attn.weight\n",
      "transformer.h.1.attn.c_attn.bias\n",
      "transformer.h.1.attn.c_proj.weight\n",
      "transformer.h.1.attn.c_proj.bias\n",
      "transformer.h.1.ln_2.weight\n",
      "transformer.h.1.ln_2.bias\n",
      "transformer.h.1.crossattention.c_attn.weight\n",
      "transformer.h.1.crossattention.c_attn.bias\n",
      "transformer.h.1.crossattention.q_attn.weight\n",
      "transformer.h.1.crossattention.q_attn.bias\n",
      "transformer.h.1.crossattention.c_proj.weight\n",
      "transformer.h.1.crossattention.c_proj.bias\n",
      "transformer.h.1.ln_cross_attn.weight\n",
      "transformer.h.1.ln_cross_attn.bias\n",
      "transformer.h.1.mlp.c_fc.weight\n",
      "transformer.h.1.mlp.c_fc.bias\n",
      "transformer.h.1.mlp.c_proj.weight\n",
      "transformer.h.1.mlp.c_proj.bias\n",
      "transformer.h.2.ln_1.weight\n",
      "transformer.h.2.ln_1.bias\n",
      "transformer.h.2.attn.c_attn.weight\n",
      "transformer.h.2.attn.c_attn.bias\n",
      "transformer.h.2.attn.c_proj.weight\n",
      "transformer.h.2.attn.c_proj.bias\n",
      "transformer.h.2.ln_2.weight\n",
      "transformer.h.2.ln_2.bias\n",
      "transformer.h.2.crossattention.c_attn.weight\n",
      "transformer.h.2.crossattention.c_attn.bias\n",
      "transformer.h.2.crossattention.q_attn.weight\n",
      "transformer.h.2.crossattention.q_attn.bias\n",
      "transformer.h.2.crossattention.c_proj.weight\n",
      "transformer.h.2.crossattention.c_proj.bias\n",
      "transformer.h.2.ln_cross_attn.weight\n",
      "transformer.h.2.ln_cross_attn.bias\n",
      "transformer.h.2.mlp.c_fc.weight\n",
      "transformer.h.2.mlp.c_fc.bias\n",
      "transformer.h.2.mlp.c_proj.weight\n",
      "transformer.h.2.mlp.c_proj.bias\n",
      "transformer.h.3.ln_1.weight\n",
      "transformer.h.3.ln_1.bias\n",
      "transformer.h.3.attn.c_attn.weight\n",
      "transformer.h.3.attn.c_attn.bias\n",
      "transformer.h.3.attn.c_proj.weight\n",
      "transformer.h.3.attn.c_proj.bias\n",
      "transformer.h.3.ln_2.weight\n",
      "transformer.h.3.ln_2.bias\n",
      "transformer.h.3.crossattention.c_attn.weight\n",
      "transformer.h.3.crossattention.c_attn.bias\n",
      "transformer.h.3.crossattention.q_attn.weight\n",
      "transformer.h.3.crossattention.q_attn.bias\n",
      "transformer.h.3.crossattention.c_proj.weight\n",
      "transformer.h.3.crossattention.c_proj.bias\n",
      "transformer.h.3.ln_cross_attn.weight\n",
      "transformer.h.3.ln_cross_attn.bias\n",
      "transformer.h.3.mlp.c_fc.weight\n",
      "transformer.h.3.mlp.c_fc.bias\n",
      "transformer.h.3.mlp.c_proj.weight\n",
      "transformer.h.3.mlp.c_proj.bias\n",
      "transformer.h.4.ln_1.weight\n",
      "transformer.h.4.ln_1.bias\n",
      "transformer.h.4.attn.c_attn.weight\n",
      "transformer.h.4.attn.c_attn.bias\n",
      "transformer.h.4.attn.c_proj.weight\n",
      "transformer.h.4.attn.c_proj.bias\n",
      "transformer.h.4.ln_2.weight\n",
      "transformer.h.4.ln_2.bias\n",
      "transformer.h.4.crossattention.c_attn.weight\n",
      "transformer.h.4.crossattention.c_attn.bias\n",
      "transformer.h.4.crossattention.q_attn.weight\n",
      "transformer.h.4.crossattention.q_attn.bias\n",
      "transformer.h.4.crossattention.c_proj.weight\n",
      "transformer.h.4.crossattention.c_proj.bias\n",
      "transformer.h.4.ln_cross_attn.weight\n",
      "transformer.h.4.ln_cross_attn.bias\n",
      "transformer.h.4.mlp.c_fc.weight\n",
      "transformer.h.4.mlp.c_fc.bias\n",
      "transformer.h.4.mlp.c_proj.weight\n",
      "transformer.h.4.mlp.c_proj.bias\n",
      "transformer.h.5.ln_1.weight\n",
      "transformer.h.5.ln_1.bias\n",
      "transformer.h.5.attn.c_attn.weight\n",
      "transformer.h.5.attn.c_attn.bias\n",
      "transformer.h.5.attn.c_proj.weight\n",
      "transformer.h.5.attn.c_proj.bias\n",
      "transformer.h.5.ln_2.weight\n",
      "transformer.h.5.ln_2.bias\n",
      "transformer.h.5.crossattention.c_attn.weight\n",
      "transformer.h.5.crossattention.c_attn.bias\n",
      "transformer.h.5.crossattention.q_attn.weight\n",
      "transformer.h.5.crossattention.q_attn.bias\n",
      "transformer.h.5.crossattention.c_proj.weight\n",
      "transformer.h.5.crossattention.c_proj.bias\n",
      "transformer.h.5.ln_cross_attn.weight\n",
      "transformer.h.5.ln_cross_attn.bias\n",
      "transformer.h.5.mlp.c_fc.weight\n",
      "transformer.h.5.mlp.c_fc.bias\n",
      "transformer.h.5.mlp.c_proj.weight\n",
      "transformer.h.5.mlp.c_proj.bias\n",
      "transformer.ln_f.weight\n",
      "transformer.ln_f.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.decoder.named_parameters():\n",
    "    print(name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Freeze layers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7b61610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze layer of the encoder (Assuming that the model is already good at understanding images). Since all cross-attention weights need to be optimized\n",
    "# I do not freeze any of the decoder layers.\n",
    "for name, param in model.encoder.named_parameters():\n",
    "    # freeze stage 1 and 2 of the Swin encoder.\n",
    "    if 'encoder.layer.3' in name:\n",
    "        break\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "- To decode using GPT's tokenizer I need to pass the token ids"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"rouge\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# throw this error --> TypeError: argument 'ids': 'list' object cannot be interpreted as an integer\n",
    "import numpy as np\n",
    "\n",
    "ignore_pad_token_for_loss = True\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "\n",
    "\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"\n",
    "     Process the predicted captions and the labels (image captions) to use the feed them to the Rough Metric to compute evaluation metrics.\n",
    "    :param eval_preds: set of predicted and target tokens. Type: transformers.trainer_utils.EvalPrediction\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print(f'-------------{type(eval_preds)}----------')\n",
    "    print(f'-------------{eval_preds.__dict__.keys()}----------')\n",
    "    print(f'-------------{eval_preds.__dict__[\"label_ids\"]}----------')\n",
    "    print(f'-------------{eval_preds.__dict__[\"inputs\"]}----------')\n",
    "\n",
    "\n",
    "\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        # numpy.ndarray\n",
    "        preds = preds[0]\n",
    "        print('------it is a tupple-------')\n",
    "        print(f'-------------{preds.size}----------')\n",
    "\n",
    "\n",
    "\n",
    "    #predicted captions are decoded into strings using GPT-2 tokenizer\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    if ignore_pad_token_for_loss:\n",
    "        # the token ID -100 indicates the end of the sequence.\n",
    "        # Replaces all -100 values with the id of the padding token in the tokeniezer\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # decoded strings are furthered pre-preprocessed e.g., split them into sentences using `sentence_tokenize` from NLTK\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds,\n",
    "                                                     decoded_labels)\n",
    "    # Compute the Rough metric. The metric uses stemming to match words with the same root.\n",
    "    result = metric.compute(predictions=decoded_preds,\n",
    "                            references=decoded_labels,\n",
    "                            use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n",
    "    ]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Procedure:\n",
    "\n",
    "- The swin-gpt-2-image-captioning model is trained with the `Trainer` class from the transformers library.\n",
    "- The arguments used for training are set via `TrainingArguments` class.\n",
    "- The training is started by the `train ` method of the trainer class."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6825c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo look into what this data collector does\n",
    "#from transformers import default_data_collator\n",
    "training_arg = TrainingArguments(\n",
    "    output_dir='../models/swin_image_captioning', # The output directory\n",
    "    overwrite_output_dir=True, # overwrite the content of the output directory\n",
    "    num_train_epochs=2, # number of training epochs\n",
    "    per_device_train_batch_size=64, # batch size for training\n",
    "    per_device_eval_batch_size=64,  # batch size for evaluation\n",
    "    load_best_model_at_end=True,\n",
    "    log_level='info',\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arg,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=processed_dataset['train'],\n",
    "    eval_dataset=processed_dataset['test'],\n",
    "    #data_collator=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2eb998fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `VisionEncoderDecoderModel.forward` and have been ignored: image_id, file_name, coco_url, caption_id, width, height. If image_id, file_name, coco_url, caption_id, width, height are not expected by `VisionEncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.evaluate()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0eed88a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `VisionEncoderDecoderModel.forward` and have been ignored: image_id, file_name, coco_url, caption_id, width, height. If image_id, file_name, coco_url, caption_id, width, height are not expected by `VisionEncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "/Users/yesidcano/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 54\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2/2 : < :, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `VisionEncoderDecoderModel.forward` and have been ignored: image_id, file_name, coco_url, caption_id, width, height. If image_id, file_name, coco_url, caption_id, width, height are not expected by `VisionEncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ../models/swin_image_captioning/checkpoint-1\n",
      "Configuration saved in ../models/swin_image_captioning/checkpoint-1/config.json\n",
      "Model weights saved in ../models/swin_image_captioning/checkpoint-1/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `VisionEncoderDecoderModel.forward` and have been ignored: image_id, file_name, coco_url, caption_id, width, height. If image_id, file_name, coco_url, caption_id, width, height are not expected by `VisionEncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ../models/swin_image_captioning/checkpoint-2\n",
      "Configuration saved in ../models/swin_image_captioning/checkpoint-2/config.json\n",
      "Model weights saved in ../models/swin_image_captioning/checkpoint-2/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ../models/swin_image_captioning/checkpoint-2 (score: 5.221934795379639).\n"
     ]
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=2, training_loss=5.324127197265625, metrics={'train_runtime': 18.2559, 'train_samples_per_second': 5.916, 'train_steps_per_second': 0.11, 'total_flos': 8216125589274624.0, 'train_loss': 5.324127197265625, 'epoch': 2.0})"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 epochs took 8 hours\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# predictions = trainer.predict(processed_dataset['test'])\n",
    "# for p in predictions.label_ids:\n",
    "#\n",
    "#     print(p)\n",
    "#     # keeping the -100 throws this error --> OverflowError: out of range integral type conversion attempted\n",
    "#     to_decode = np.where(p != -100, p, tokenizer.pad_token_id)\n",
    "#     print(to_decode)\n",
    "#     decoded = tokenizer.decode(to_decode, skip_special_tokens=True)\n",
    "#     print(decoded)\n",
    "#     break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `VisionEncoderDecoderModel.forward` and have been ignored: image_id, file_name, coco_url, caption_id, width, height. If image_id, file_name, coco_url, caption_id, width, height are not expected by `VisionEncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 6\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[34], line 7\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m#use logits to convert the prediction\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Calculate the Bleu score for the test set\u001B[39;00m\n\u001B[1;32m      6\u001B[0m predictions \u001B[38;5;241m=\u001B[39m trainer\u001B[38;5;241m.\u001B[39mpredict(processed_dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m----> 7\u001B[0m predicted_label_ids \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241m.\u001B[39margmax(predictions\u001B[38;5;241m.\u001B[39mpredictions, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39msqueeze()\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[1;32m      8\u001B[0m to_decode \u001B[38;5;241m=\u001B[39m [np\u001B[38;5;241m.\u001B[39mwhere(p \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m100\u001B[39m, p, tokenizer\u001B[38;5;241m.\u001B[39mpad_token_id) \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m predictions\u001B[38;5;241m.\u001B[39mlabel_ids]\n\u001B[1;32m      9\u001B[0m predicted_captions \u001B[38;5;241m=\u001B[39m [tokenizer\u001B[38;5;241m.\u001B[39mdecode(p, skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m to_decode]\n",
      "Cell \u001B[0;32mIn[34], line 7\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m#use logits to convert the prediction\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Calculate the Bleu score for the test set\u001B[39;00m\n\u001B[1;32m      6\u001B[0m predictions \u001B[38;5;241m=\u001B[39m trainer\u001B[38;5;241m.\u001B[39mpredict(processed_dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m----> 7\u001B[0m predicted_label_ids \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241m.\u001B[39margmax(predictions\u001B[38;5;241m.\u001B[39mpredictions, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39msqueeze()\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[1;32m      8\u001B[0m to_decode \u001B[38;5;241m=\u001B[39m [np\u001B[38;5;241m.\u001B[39mwhere(p \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m100\u001B[39m, p, tokenizer\u001B[38;5;241m.\u001B[39mpad_token_id) \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m predictions\u001B[38;5;241m.\u001B[39mlabel_ids]\n\u001B[1;32m      9\u001B[0m predicted_captions \u001B[38;5;241m=\u001B[39m [tokenizer\u001B[38;5;241m.\u001B[39mdecode(p, skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m to_decode]\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py:747\u001B[0m, in \u001B[0;36mPyDBFrame.trace_dispatch\u001B[0;34m(self, frame, event, arg)\u001B[0m\n\u001B[1;32m    745\u001B[0m \u001B[38;5;66;03m# if thread has a suspend flag, we suspend with a busy wait\u001B[39;00m\n\u001B[1;32m    746\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m info\u001B[38;5;241m.\u001B[39mpydev_state \u001B[38;5;241m==\u001B[39m STATE_SUSPEND:\n\u001B[0;32m--> 747\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    748\u001B[0m     \u001B[38;5;66;03m# No need to reset frame.f_trace to keep the same trace function.\u001B[39;00m\n\u001B[1;32m    749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrace_dispatch\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py:144\u001B[0m, in \u001B[0;36mPyDBFrame.do_wait_suspend\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdo_wait_suspend\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 144\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_args\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py:1147\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1144\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[1;32m   1146\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[0;32m-> 1147\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py:1162\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1159\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[1;32m   1161\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[0;32m-> 1162\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1164\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[1;32m   1166\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import nltk.translate.bleu_score as bleu\n",
    "\n",
    "#use logits to convert the prediction\n",
    "\n",
    "# Calculate the Bleu score for the test set\n",
    "predictions = trainer.predict(processed_dataset['test'])\n",
    "predicted_label_ids = torch.argmax(predictions.predictions, dim=-1).squeeze().tolist()\n",
    "to_decode = [np.where(p != -100, p, tokenizer.pad_token_id) for p in predictions.label_ids]\n",
    "predicted_captions = [tokenizer.decode(p, skip_special_tokens=True) for p in to_decode]\n",
    "\n",
    "def to_np (r):\n",
    "    r = np.array(r)\n",
    "    r = np.where(r != -100, r, tokenizer.pad_token_id)\n",
    "    return r\n",
    "\n",
    "reference_captions = processed_dataset['test']['labels']\n",
    "to_decode_labels = [to_np (r) for r in reference_captions]\n",
    "reference_captions_decoded = [tokenizer.decode(l, skip_special_tokens=True) for l in to_decode_labels]\n",
    "\n",
    "\n",
    "bleu_score = bleu.corpus_bleu([[r] for r in reference_captions_decoded], predicted_captions)\n",
    "\n",
    "\n",
    "print(\"Bleu Score:\", bleu_score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gkg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c1ef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#need to save the tokenizer\n",
    "tokenizer.save_pretrained('../models/swin_image_captioning')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc3b39e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# loading model and config from pretrained folder\n",
    "finetuned_model = VisionEncoderDecoderModel.from_pretrained('../models/swin_image_captioning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Make prediction using the pipeline library\n",
    "look into wheater you can pass the feature_extractor as a parameter as well.\n",
    "do_sample: returns multiple options\n",
    "top_p: getting more confident less random, sharpen predictions\n",
    "\n",
    "pretrained_generator = pipeline(\n",
    "    'text-generation', model=model, tokenizer='gpt2',\n",
    "    config={'max_length': 200, 'do_sample': True, 'top_p': 0.9, 'temperature': 0.7, 'top_k': 10}\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from IPython.core.display_functions import display\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inference_transforms = transforms.Compose(\n",
    "    [\n",
    "\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# a helper function to caption images from the web or a file path\n",
    "def generate_caption(m, path):\n",
    "\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    img_transformed = inference_transforms(img).unsqueeze(0)\n",
    "# tensor dimensions max_lenght X num_return_sequences, where ij == some_token_id\n",
    "    model_output = m.generate(\n",
    "        img_transformed,\n",
    "        num_beams=3,\n",
    "        max_length=15,\n",
    "        early_stopping=True,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=5,\n",
    "    )\n",
    "    # g is a tensor like this one: tensor([50256,    13,   198,   198,   198,   198,   198,   198,   198, 50256,\n",
    "    #50256, 50256, 50256, 50256, 50256])\n",
    "    captions = [tokenizer.decode(g, skip_special_tokens=True).strip() for g in model_output]\n",
    "    #Show image\n",
    "    display(img)\n",
    "    return captions, model_output, img_transformed\n",
    "\n",
    "\n",
    "captions, model_output, img_transformed = generate_caption(  # Out of sample photo\n",
    "    finetuned_model, '../data/test_data/000000421195_test.jpg'\n",
    ")\n",
    "\n",
    "captions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#from transformers import pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# image_captioner = pipeline(\"image-to-text\", model=\"../models/swin_image_captioning\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}