{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Image-captioning model\n",
    "\n",
    "### Overview\n",
    "\n",
    "The code below trains an image captioning algorithm using a pre-trained vision and language transformer as well as the COCO dataset. The model uses the Microsoft Swin-Tiny-Patch4-Window7-224 and DistilGPT-2  pre-trained models as the encoder and decoder respectively.\n",
    "The code implementation is done with the help of the following libraries/packages:\n",
    " - Transformers library from Hugging Face to implement the model.\n",
    " - PyTorch for tensor operations\n",
    " - PIL and torchvision libraries for image processing.\n",
    "\n",
    "\n",
    "### Instructions to Run this notebook\n",
    "\n",
    "To run this code on a local machine, follow these steps:\n",
    "\n",
    "- Install the requirements listed in `requirements.txt`. Make sure to include the Transformers, datasets, scikit-learn, PIL, and PyTorch libraries/packages\n",
    "\n",
    "- Download the COCO dataset and place it in the directory specified by the COCO_DIR variable.\n",
    "\n",
    "Run the code in a Python environment that has access to a GPU. The code can be run in a Jupyter notebook or in a Python script. This code was run on MacBook M1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderModel, GPT2TokenizerFast, AutoFeatureExtractor, \\\n",
    "    TrainingArguments, Trainer\n",
    "# todo look into what this data collector does\n",
    "#from transformers import default_data_collator\n",
    "\n",
    "import datasets\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from IPython.core.display_functions import display\n",
    "import os\n",
    "\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Device\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "RadioButtons(description='Select device', options=('M1', 'Other'), value='M1')",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0cd210fc1b454fdaa59c5382ff567fc2"
      }
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_type = widgets.RadioButtons(\n",
    "    options=['M1', 'Other'],\n",
    "    value='M1',\n",
    "    description='Select device',\n",
    "    disabled=False\n",
    ")\n",
    "device_type"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train on gpu\n",
    "if device_type.value == 'M1':\n",
    "\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "else:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# test mps device support\n",
    "torch.has_mps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that the cross-attention layers will be randomly initialized (see the warning message below) and hence they need to be fine-tuned on a downstream task, in thi case, image captioning generation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yesidcano/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1673856676759/work/aten/src/ATen/native/TensorShape.cpp:3454.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Some weights of the model checkpoint at microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['transformer.h.0.crossattention.q_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.5.crossattention.bias', 'transformer.h.4.crossattention.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.4.crossattention.masked_bias', 'transformer.h.0.crossattention.bias', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.3.crossattention.masked_bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.0.crossattention.masked_bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.2.crossattention.masked_bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.1.crossattention.masked_bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.3.crossattention.bias', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.5.crossattention.masked_bias', 'transformer.h.1.crossattention.bias', 'transformer.h.2.crossattention.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.crossattention.c_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model uses a rre-trained encoder of type <class 'transformers.models.swin.modeling_swin.SwinModel'> and pre-trained decoder of type <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#One of the main objectives is optimizing the cross-attention weights. How GPT connects to the encoders output,\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    'microsoft/swin-tiny-patch4-window7-224',\n",
    "    'distilgpt2'\n",
    ").to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Encoder and Decoder pretrained Models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'This model uses a pre-trained encoder of type {type(model.encoder)} and pre-trained decoder of type {type(model.decoder)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Number of Parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4e9f1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 123,615,354\n"
     ]
    }
   ],
   "source": [
    "# Vit and GPT2 have 182,485,248 combined parameters. Swin version I am tuning here has fewer parameters\n",
    "from torch import numel\n",
    "\n",
    "combined_params = 0\n",
    "for param in model.parameters():\n",
    "    combined_params += numel(param)\n",
    "    \n",
    "print(f\"Total number of parameters: {combined_params:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Database\n",
    "The image captioning algorithm is trained on the COCO dataset which consists of images and captions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#Loads a small sample of the COCO database\n",
    "# import datasets\n",
    "# ds = datasets.load_dataset(\"ydshieh/coco_dataset_script\", \"2017\", data_dir=\"./dummy_data/\")\n",
    "# ds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# COCO_DIR = input('Path to COCO dataset')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset coco_dataset_script (/Users/yesidcano/.cache/huggingface/datasets/ydshieh___coco_dataset_script/2017-6b5176efb5303df4/0.0.0/e033205c0266a54c10be132f9264f2a39dcf893e798f6756d224b1ff5078998f)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c77c15ea33c14325a390c698a00c584e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['image_id', 'caption_id', 'caption', 'height', 'width', 'file_name', 'coco_url', 'image_path'],\n        num_rows: 591753\n    })\n    validation: Dataset({\n        features: ['image_id', 'caption_id', 'caption', 'height', 'width', 'file_name', 'coco_url', 'image_path'],\n        num_rows: 25014\n    })\n    test: Dataset({\n        features: ['image_id', 'caption_id', 'caption', 'height', 'width', 'file_name', 'coco_url', 'image_path'],\n        num_rows: 40670\n    })\n})"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Todo test if the image ids are repeated. the bumber of rows is given by the caption not by the images. The same image can have several descriptions, hence these\n",
    "# several data points.\n",
    "\"\"\"\"\n",
    "1. Determine the number of unique ids\n",
    "2. Determine the average captions per image\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#the datasets.load_dataset manages everything related to caching. So I have to use it.\n",
    "COCO_DIR = '/Users/yesidcano/repos/image-captioning/data/coco'\n",
    "\n",
    "\n",
    "#ds = datasets.load_dataset(\"ydshieh/coco_dataset_script\", \"2017\",data_dir=COCO_DIR, cache_dir='/Users/yesidcano/repos/db_coco_cache')\n",
    "\n",
    "# Load a slice of the database this https://huggingface.co/docs/datasets/loading to split dataset.\n",
    "\n",
    "ds = datasets.load_dataset(\"ydshieh/coco_dataset_script\", \"2017\",data_dir=COCO_DIR)\n",
    "# ds = datasets.load_dataset(\"ydshieh/coco_dataset_script\", \"2017\",data_dir=COCO_DIR, split=\"train[10:70]\")\n",
    "ds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4450a9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
    "#gpt-2 tokenizer\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('distilgpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Model config\n",
    "\n",
    "model.config.pad_token = tokenizer.pad_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "model.config.decoder_start_token = tokenizer.bos_token\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "ViTFeatureExtractor {\n  \"do_normalize\": true,\n  \"do_resize\": true,\n  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n  \"image_mean\": [\n    0.485,\n    0.456,\n    0.406\n  ],\n  \"image_std\": [\n    0.229,\n    0.224,\n    0.225\n  ],\n  \"resample\": 3,\n  \"size\": 224\n}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uses fast tokenizer\n",
    "tokenizer.is_fast"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Define the transforms to be applied to the images\n",
    "transform = transforms.Compose([\n",
    "\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/yesidcano/.cache/huggingface/datasets/ydshieh___coco_dataset_script/2017-6b5176efb5303df4/0.0.0/e033205c0266a54c10be132f9264f2a39dcf893e798f6756d224b1ff5078998f/cache-3bbabcbb08e58764.arrow\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def preprocess_fn(examples):\n",
    "    # Swin expects pixel_values instead of input_ids\n",
    "    examples['pixel_values'] = [transform(Image.open(path).convert('RGB')) for path in examples['image_path']]\n",
    "    # todo set this parameter to the average length of the captions\n",
    "    tokenized = tokenizer(\n",
    "        examples['caption'], padding='max_length', max_length=50, truncation=True\n",
    "    )['input_ids']\n",
    "\n",
    "    # the output captions\n",
    "    examples['labels'] = [[l if l != tokenizer.pad_token_id else -100 for l in t] for t in tokenized]\n",
    "\n",
    "    # delete unused keys\n",
    "    del examples['image_path']\n",
    "    del examples['caption']\n",
    "    return examples\n",
    "\n",
    "\n",
    "processed_dataset = ds.map(\n",
    "    function=preprocess_fn,\n",
    "    batched=True,\n",
    "    batch_size=50,\n",
    "    # remove_columns=['image_id', 'caption_id', 'caption', 'height', 'width', 'file_name', 'coco_url', 'image_path']\n",
    "\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5ca503c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['image_id', 'caption_id', 'height', 'width', 'file_name', 'coco_url', 'pixel_values', 'labels'],\n        num_rows: 54\n    })\n    test: Dataset({\n        features: ['image_id', 'caption_id', 'height', 'width', 'file_name', 'coco_url', 'pixel_values', 'labels'],\n        num_rows: 6\n    })\n})"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By default data are shuffled.\n",
    "processed_dataset = processed_dataset.train_test_split(test_size=0.1)\n",
    "processed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model's layers\n",
    "Below a list of the encoder and decoder's layers respectively."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Encoder Layers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.patch_embeddings.projection.weight\n",
      "embeddings.patch_embeddings.projection.bias\n",
      "embeddings.norm.weight\n",
      "embeddings.norm.bias\n",
      "encoder.layers.0.blocks.0.layernorm_before.weight\n",
      "encoder.layers.0.blocks.0.layernorm_before.bias\n",
      "encoder.layers.0.blocks.0.attention.self.relative_position_bias_table\n",
      "encoder.layers.0.blocks.0.attention.self.query.weight\n",
      "encoder.layers.0.blocks.0.attention.self.query.bias\n",
      "encoder.layers.0.blocks.0.attention.self.key.weight\n",
      "encoder.layers.0.blocks.0.attention.self.key.bias\n",
      "encoder.layers.0.blocks.0.attention.self.value.weight\n",
      "encoder.layers.0.blocks.0.attention.self.value.bias\n",
      "encoder.layers.0.blocks.0.attention.output.dense.weight\n",
      "encoder.layers.0.blocks.0.attention.output.dense.bias\n",
      "encoder.layers.0.blocks.0.layernorm_after.weight\n",
      "encoder.layers.0.blocks.0.layernorm_after.bias\n",
      "encoder.layers.0.blocks.0.intermediate.dense.weight\n",
      "encoder.layers.0.blocks.0.intermediate.dense.bias\n",
      "encoder.layers.0.blocks.0.output.dense.weight\n",
      "encoder.layers.0.blocks.0.output.dense.bias\n",
      "encoder.layers.0.blocks.1.layernorm_before.weight\n",
      "encoder.layers.0.blocks.1.layernorm_before.bias\n",
      "encoder.layers.0.blocks.1.attention.self.relative_position_bias_table\n",
      "encoder.layers.0.blocks.1.attention.self.query.weight\n",
      "encoder.layers.0.blocks.1.attention.self.query.bias\n",
      "encoder.layers.0.blocks.1.attention.self.key.weight\n",
      "encoder.layers.0.blocks.1.attention.self.key.bias\n",
      "encoder.layers.0.blocks.1.attention.self.value.weight\n",
      "encoder.layers.0.blocks.1.attention.self.value.bias\n",
      "encoder.layers.0.blocks.1.attention.output.dense.weight\n",
      "encoder.layers.0.blocks.1.attention.output.dense.bias\n",
      "encoder.layers.0.blocks.1.layernorm_after.weight\n",
      "encoder.layers.0.blocks.1.layernorm_after.bias\n",
      "encoder.layers.0.blocks.1.intermediate.dense.weight\n",
      "encoder.layers.0.blocks.1.intermediate.dense.bias\n",
      "encoder.layers.0.blocks.1.output.dense.weight\n",
      "encoder.layers.0.blocks.1.output.dense.bias\n",
      "encoder.layers.0.downsample.reduction.weight\n",
      "encoder.layers.0.downsample.norm.weight\n",
      "encoder.layers.0.downsample.norm.bias\n",
      "encoder.layers.1.blocks.0.layernorm_before.weight\n",
      "encoder.layers.1.blocks.0.layernorm_before.bias\n",
      "encoder.layers.1.blocks.0.attention.self.relative_position_bias_table\n",
      "encoder.layers.1.blocks.0.attention.self.query.weight\n",
      "encoder.layers.1.blocks.0.attention.self.query.bias\n",
      "encoder.layers.1.blocks.0.attention.self.key.weight\n",
      "encoder.layers.1.blocks.0.attention.self.key.bias\n",
      "encoder.layers.1.blocks.0.attention.self.value.weight\n",
      "encoder.layers.1.blocks.0.attention.self.value.bias\n",
      "encoder.layers.1.blocks.0.attention.output.dense.weight\n",
      "encoder.layers.1.blocks.0.attention.output.dense.bias\n",
      "encoder.layers.1.blocks.0.layernorm_after.weight\n",
      "encoder.layers.1.blocks.0.layernorm_after.bias\n",
      "encoder.layers.1.blocks.0.intermediate.dense.weight\n",
      "encoder.layers.1.blocks.0.intermediate.dense.bias\n",
      "encoder.layers.1.blocks.0.output.dense.weight\n",
      "encoder.layers.1.blocks.0.output.dense.bias\n",
      "encoder.layers.1.blocks.1.layernorm_before.weight\n",
      "encoder.layers.1.blocks.1.layernorm_before.bias\n",
      "encoder.layers.1.blocks.1.attention.self.relative_position_bias_table\n",
      "encoder.layers.1.blocks.1.attention.self.query.weight\n",
      "encoder.layers.1.blocks.1.attention.self.query.bias\n",
      "encoder.layers.1.blocks.1.attention.self.key.weight\n",
      "encoder.layers.1.blocks.1.attention.self.key.bias\n",
      "encoder.layers.1.blocks.1.attention.self.value.weight\n",
      "encoder.layers.1.blocks.1.attention.self.value.bias\n",
      "encoder.layers.1.blocks.1.attention.output.dense.weight\n",
      "encoder.layers.1.blocks.1.attention.output.dense.bias\n",
      "encoder.layers.1.blocks.1.layernorm_after.weight\n",
      "encoder.layers.1.blocks.1.layernorm_after.bias\n",
      "encoder.layers.1.blocks.1.intermediate.dense.weight\n",
      "encoder.layers.1.blocks.1.intermediate.dense.bias\n",
      "encoder.layers.1.blocks.1.output.dense.weight\n",
      "encoder.layers.1.blocks.1.output.dense.bias\n",
      "encoder.layers.1.downsample.reduction.weight\n",
      "encoder.layers.1.downsample.norm.weight\n",
      "encoder.layers.1.downsample.norm.bias\n",
      "encoder.layers.2.blocks.0.layernorm_before.weight\n",
      "encoder.layers.2.blocks.0.layernorm_before.bias\n",
      "encoder.layers.2.blocks.0.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.0.attention.self.query.weight\n",
      "encoder.layers.2.blocks.0.attention.self.query.bias\n",
      "encoder.layers.2.blocks.0.attention.self.key.weight\n",
      "encoder.layers.2.blocks.0.attention.self.key.bias\n",
      "encoder.layers.2.blocks.0.attention.self.value.weight\n",
      "encoder.layers.2.blocks.0.attention.self.value.bias\n",
      "encoder.layers.2.blocks.0.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.0.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.0.layernorm_after.weight\n",
      "encoder.layers.2.blocks.0.layernorm_after.bias\n",
      "encoder.layers.2.blocks.0.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.0.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.0.output.dense.weight\n",
      "encoder.layers.2.blocks.0.output.dense.bias\n",
      "encoder.layers.2.blocks.1.layernorm_before.weight\n",
      "encoder.layers.2.blocks.1.layernorm_before.bias\n",
      "encoder.layers.2.blocks.1.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.1.attention.self.query.weight\n",
      "encoder.layers.2.blocks.1.attention.self.query.bias\n",
      "encoder.layers.2.blocks.1.attention.self.key.weight\n",
      "encoder.layers.2.blocks.1.attention.self.key.bias\n",
      "encoder.layers.2.blocks.1.attention.self.value.weight\n",
      "encoder.layers.2.blocks.1.attention.self.value.bias\n",
      "encoder.layers.2.blocks.1.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.1.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.1.layernorm_after.weight\n",
      "encoder.layers.2.blocks.1.layernorm_after.bias\n",
      "encoder.layers.2.blocks.1.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.1.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.1.output.dense.weight\n",
      "encoder.layers.2.blocks.1.output.dense.bias\n",
      "encoder.layers.2.blocks.2.layernorm_before.weight\n",
      "encoder.layers.2.blocks.2.layernorm_before.bias\n",
      "encoder.layers.2.blocks.2.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.2.attention.self.query.weight\n",
      "encoder.layers.2.blocks.2.attention.self.query.bias\n",
      "encoder.layers.2.blocks.2.attention.self.key.weight\n",
      "encoder.layers.2.blocks.2.attention.self.key.bias\n",
      "encoder.layers.2.blocks.2.attention.self.value.weight\n",
      "encoder.layers.2.blocks.2.attention.self.value.bias\n",
      "encoder.layers.2.blocks.2.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.2.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.2.layernorm_after.weight\n",
      "encoder.layers.2.blocks.2.layernorm_after.bias\n",
      "encoder.layers.2.blocks.2.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.2.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.2.output.dense.weight\n",
      "encoder.layers.2.blocks.2.output.dense.bias\n",
      "encoder.layers.2.blocks.3.layernorm_before.weight\n",
      "encoder.layers.2.blocks.3.layernorm_before.bias\n",
      "encoder.layers.2.blocks.3.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.3.attention.self.query.weight\n",
      "encoder.layers.2.blocks.3.attention.self.query.bias\n",
      "encoder.layers.2.blocks.3.attention.self.key.weight\n",
      "encoder.layers.2.blocks.3.attention.self.key.bias\n",
      "encoder.layers.2.blocks.3.attention.self.value.weight\n",
      "encoder.layers.2.blocks.3.attention.self.value.bias\n",
      "encoder.layers.2.blocks.3.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.3.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.3.layernorm_after.weight\n",
      "encoder.layers.2.blocks.3.layernorm_after.bias\n",
      "encoder.layers.2.blocks.3.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.3.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.3.output.dense.weight\n",
      "encoder.layers.2.blocks.3.output.dense.bias\n",
      "encoder.layers.2.blocks.4.layernorm_before.weight\n",
      "encoder.layers.2.blocks.4.layernorm_before.bias\n",
      "encoder.layers.2.blocks.4.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.4.attention.self.query.weight\n",
      "encoder.layers.2.blocks.4.attention.self.query.bias\n",
      "encoder.layers.2.blocks.4.attention.self.key.weight\n",
      "encoder.layers.2.blocks.4.attention.self.key.bias\n",
      "encoder.layers.2.blocks.4.attention.self.value.weight\n",
      "encoder.layers.2.blocks.4.attention.self.value.bias\n",
      "encoder.layers.2.blocks.4.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.4.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.4.layernorm_after.weight\n",
      "encoder.layers.2.blocks.4.layernorm_after.bias\n",
      "encoder.layers.2.blocks.4.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.4.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.4.output.dense.weight\n",
      "encoder.layers.2.blocks.4.output.dense.bias\n",
      "encoder.layers.2.blocks.5.layernorm_before.weight\n",
      "encoder.layers.2.blocks.5.layernorm_before.bias\n",
      "encoder.layers.2.blocks.5.attention.self.relative_position_bias_table\n",
      "encoder.layers.2.blocks.5.attention.self.query.weight\n",
      "encoder.layers.2.blocks.5.attention.self.query.bias\n",
      "encoder.layers.2.blocks.5.attention.self.key.weight\n",
      "encoder.layers.2.blocks.5.attention.self.key.bias\n",
      "encoder.layers.2.blocks.5.attention.self.value.weight\n",
      "encoder.layers.2.blocks.5.attention.self.value.bias\n",
      "encoder.layers.2.blocks.5.attention.output.dense.weight\n",
      "encoder.layers.2.blocks.5.attention.output.dense.bias\n",
      "encoder.layers.2.blocks.5.layernorm_after.weight\n",
      "encoder.layers.2.blocks.5.layernorm_after.bias\n",
      "encoder.layers.2.blocks.5.intermediate.dense.weight\n",
      "encoder.layers.2.blocks.5.intermediate.dense.bias\n",
      "encoder.layers.2.blocks.5.output.dense.weight\n",
      "encoder.layers.2.blocks.5.output.dense.bias\n",
      "encoder.layers.2.downsample.reduction.weight\n",
      "encoder.layers.2.downsample.norm.weight\n",
      "encoder.layers.2.downsample.norm.bias\n",
      "encoder.layers.3.blocks.0.layernorm_before.weight\n",
      "encoder.layers.3.blocks.0.layernorm_before.bias\n",
      "encoder.layers.3.blocks.0.attention.self.relative_position_bias_table\n",
      "encoder.layers.3.blocks.0.attention.self.query.weight\n",
      "encoder.layers.3.blocks.0.attention.self.query.bias\n",
      "encoder.layers.3.blocks.0.attention.self.key.weight\n",
      "encoder.layers.3.blocks.0.attention.self.key.bias\n",
      "encoder.layers.3.blocks.0.attention.self.value.weight\n",
      "encoder.layers.3.blocks.0.attention.self.value.bias\n",
      "encoder.layers.3.blocks.0.attention.output.dense.weight\n",
      "encoder.layers.3.blocks.0.attention.output.dense.bias\n",
      "encoder.layers.3.blocks.0.layernorm_after.weight\n",
      "encoder.layers.3.blocks.0.layernorm_after.bias\n",
      "encoder.layers.3.blocks.0.intermediate.dense.weight\n",
      "encoder.layers.3.blocks.0.intermediate.dense.bias\n",
      "encoder.layers.3.blocks.0.output.dense.weight\n",
      "encoder.layers.3.blocks.0.output.dense.bias\n",
      "encoder.layers.3.blocks.1.layernorm_before.weight\n",
      "encoder.layers.3.blocks.1.layernorm_before.bias\n",
      "encoder.layers.3.blocks.1.attention.self.relative_position_bias_table\n",
      "encoder.layers.3.blocks.1.attention.self.query.weight\n",
      "encoder.layers.3.blocks.1.attention.self.query.bias\n",
      "encoder.layers.3.blocks.1.attention.self.key.weight\n",
      "encoder.layers.3.blocks.1.attention.self.key.bias\n",
      "encoder.layers.3.blocks.1.attention.self.value.weight\n",
      "encoder.layers.3.blocks.1.attention.self.value.bias\n",
      "encoder.layers.3.blocks.1.attention.output.dense.weight\n",
      "encoder.layers.3.blocks.1.attention.output.dense.bias\n",
      "encoder.layers.3.blocks.1.layernorm_after.weight\n",
      "encoder.layers.3.blocks.1.layernorm_after.bias\n",
      "encoder.layers.3.blocks.1.intermediate.dense.weight\n",
      "encoder.layers.3.blocks.1.intermediate.dense.bias\n",
      "encoder.layers.3.blocks.1.output.dense.weight\n",
      "encoder.layers.3.blocks.1.output.dense.bias\n",
      "layernorm.weight\n",
      "layernorm.bias\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for name, param in model.encoder.named_parameters():\n",
    "    print(name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Decoder's layers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight\n",
      "transformer.wpe.weight\n",
      "transformer.h.0.ln_1.weight\n",
      "transformer.h.0.ln_1.bias\n",
      "transformer.h.0.attn.c_attn.weight\n",
      "transformer.h.0.attn.c_attn.bias\n",
      "transformer.h.0.attn.c_proj.weight\n",
      "transformer.h.0.attn.c_proj.bias\n",
      "transformer.h.0.ln_2.weight\n",
      "transformer.h.0.ln_2.bias\n",
      "transformer.h.0.crossattention.c_attn.weight\n",
      "transformer.h.0.crossattention.c_attn.bias\n",
      "transformer.h.0.crossattention.q_attn.weight\n",
      "transformer.h.0.crossattention.q_attn.bias\n",
      "transformer.h.0.crossattention.c_proj.weight\n",
      "transformer.h.0.crossattention.c_proj.bias\n",
      "transformer.h.0.ln_cross_attn.weight\n",
      "transformer.h.0.ln_cross_attn.bias\n",
      "transformer.h.0.mlp.c_fc.weight\n",
      "transformer.h.0.mlp.c_fc.bias\n",
      "transformer.h.0.mlp.c_proj.weight\n",
      "transformer.h.0.mlp.c_proj.bias\n",
      "transformer.h.1.ln_1.weight\n",
      "transformer.h.1.ln_1.bias\n",
      "transformer.h.1.attn.c_attn.weight\n",
      "transformer.h.1.attn.c_attn.bias\n",
      "transformer.h.1.attn.c_proj.weight\n",
      "transformer.h.1.attn.c_proj.bias\n",
      "transformer.h.1.ln_2.weight\n",
      "transformer.h.1.ln_2.bias\n",
      "transformer.h.1.crossattention.c_attn.weight\n",
      "transformer.h.1.crossattention.c_attn.bias\n",
      "transformer.h.1.crossattention.q_attn.weight\n",
      "transformer.h.1.crossattention.q_attn.bias\n",
      "transformer.h.1.crossattention.c_proj.weight\n",
      "transformer.h.1.crossattention.c_proj.bias\n",
      "transformer.h.1.ln_cross_attn.weight\n",
      "transformer.h.1.ln_cross_attn.bias\n",
      "transformer.h.1.mlp.c_fc.weight\n",
      "transformer.h.1.mlp.c_fc.bias\n",
      "transformer.h.1.mlp.c_proj.weight\n",
      "transformer.h.1.mlp.c_proj.bias\n",
      "transformer.h.2.ln_1.weight\n",
      "transformer.h.2.ln_1.bias\n",
      "transformer.h.2.attn.c_attn.weight\n",
      "transformer.h.2.attn.c_attn.bias\n",
      "transformer.h.2.attn.c_proj.weight\n",
      "transformer.h.2.attn.c_proj.bias\n",
      "transformer.h.2.ln_2.weight\n",
      "transformer.h.2.ln_2.bias\n",
      "transformer.h.2.crossattention.c_attn.weight\n",
      "transformer.h.2.crossattention.c_attn.bias\n",
      "transformer.h.2.crossattention.q_attn.weight\n",
      "transformer.h.2.crossattention.q_attn.bias\n",
      "transformer.h.2.crossattention.c_proj.weight\n",
      "transformer.h.2.crossattention.c_proj.bias\n",
      "transformer.h.2.ln_cross_attn.weight\n",
      "transformer.h.2.ln_cross_attn.bias\n",
      "transformer.h.2.mlp.c_fc.weight\n",
      "transformer.h.2.mlp.c_fc.bias\n",
      "transformer.h.2.mlp.c_proj.weight\n",
      "transformer.h.2.mlp.c_proj.bias\n",
      "transformer.h.3.ln_1.weight\n",
      "transformer.h.3.ln_1.bias\n",
      "transformer.h.3.attn.c_attn.weight\n",
      "transformer.h.3.attn.c_attn.bias\n",
      "transformer.h.3.attn.c_proj.weight\n",
      "transformer.h.3.attn.c_proj.bias\n",
      "transformer.h.3.ln_2.weight\n",
      "transformer.h.3.ln_2.bias\n",
      "transformer.h.3.crossattention.c_attn.weight\n",
      "transformer.h.3.crossattention.c_attn.bias\n",
      "transformer.h.3.crossattention.q_attn.weight\n",
      "transformer.h.3.crossattention.q_attn.bias\n",
      "transformer.h.3.crossattention.c_proj.weight\n",
      "transformer.h.3.crossattention.c_proj.bias\n",
      "transformer.h.3.ln_cross_attn.weight\n",
      "transformer.h.3.ln_cross_attn.bias\n",
      "transformer.h.3.mlp.c_fc.weight\n",
      "transformer.h.3.mlp.c_fc.bias\n",
      "transformer.h.3.mlp.c_proj.weight\n",
      "transformer.h.3.mlp.c_proj.bias\n",
      "transformer.h.4.ln_1.weight\n",
      "transformer.h.4.ln_1.bias\n",
      "transformer.h.4.attn.c_attn.weight\n",
      "transformer.h.4.attn.c_attn.bias\n",
      "transformer.h.4.attn.c_proj.weight\n",
      "transformer.h.4.attn.c_proj.bias\n",
      "transformer.h.4.ln_2.weight\n",
      "transformer.h.4.ln_2.bias\n",
      "transformer.h.4.crossattention.c_attn.weight\n",
      "transformer.h.4.crossattention.c_attn.bias\n",
      "transformer.h.4.crossattention.q_attn.weight\n",
      "transformer.h.4.crossattention.q_attn.bias\n",
      "transformer.h.4.crossattention.c_proj.weight\n",
      "transformer.h.4.crossattention.c_proj.bias\n",
      "transformer.h.4.ln_cross_attn.weight\n",
      "transformer.h.4.ln_cross_attn.bias\n",
      "transformer.h.4.mlp.c_fc.weight\n",
      "transformer.h.4.mlp.c_fc.bias\n",
      "transformer.h.4.mlp.c_proj.weight\n",
      "transformer.h.4.mlp.c_proj.bias\n",
      "transformer.h.5.ln_1.weight\n",
      "transformer.h.5.ln_1.bias\n",
      "transformer.h.5.attn.c_attn.weight\n",
      "transformer.h.5.attn.c_attn.bias\n",
      "transformer.h.5.attn.c_proj.weight\n",
      "transformer.h.5.attn.c_proj.bias\n",
      "transformer.h.5.ln_2.weight\n",
      "transformer.h.5.ln_2.bias\n",
      "transformer.h.5.crossattention.c_attn.weight\n",
      "transformer.h.5.crossattention.c_attn.bias\n",
      "transformer.h.5.crossattention.q_attn.weight\n",
      "transformer.h.5.crossattention.q_attn.bias\n",
      "transformer.h.5.crossattention.c_proj.weight\n",
      "transformer.h.5.crossattention.c_proj.bias\n",
      "transformer.h.5.ln_cross_attn.weight\n",
      "transformer.h.5.ln_cross_attn.bias\n",
      "transformer.h.5.mlp.c_fc.weight\n",
      "transformer.h.5.mlp.c_fc.bias\n",
      "transformer.h.5.mlp.c_proj.weight\n",
      "transformer.h.5.mlp.c_proj.bias\n",
      "transformer.ln_f.weight\n",
      "transformer.ln_f.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.decoder.named_parameters():\n",
    "    print(name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Freeze layers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7b61610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze layer of the encoder (Assuming that the model is already good at understanding images). Since all cross-attention weights need to be optimized\n",
    "# I do not freeze any of the decoder layers.\n",
    "for name, param in model.encoder.named_parameters():\n",
    "    # freeze stage 1 and 2 of the Swin encoder.\n",
    "    if 'encoder.layer.3' in name:\n",
    "        break\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation Metrics: Rouge and Bleu\n",
    "\n",
    "- To decode using GPT's tokenizer I need to pass the token ids"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "bleu_metric = evaluate.load(\"bleu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def metrics(pred_labels):\n",
    "    \"\"\"\n",
    "     Process the predicted captions and the labels (image captions) used to compute the metrics.\n",
    "    :param pred_labels: set of predicted and target tokens. Type: transformers.trainer_utils.EvalPrediction\n",
    "    :return: a dictionary with the resulting metric values.\n",
    "    \"\"\"\n",
    "    results={}\n",
    "    predictions = pred_labels.predictions[0]\n",
    "    labels = pred_labels.label_ids\n",
    "\n",
    "    #Convert the model's output into token IDs\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    # predicted captions are decoded into strings using GPT-2 tokenizer\n",
    "    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # the token ID -100 indicates the end of the sequence.\n",
    "    # Replaces all -100 values with the id of the padding token in the tokeniezer\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    rouge = rouge_metric.compute(predictions= decoded_predictions,\n",
    "                                         references= decoded_labels,\n",
    "                                         use_stemmer=True) # returns a dict\n",
    "\n",
    "    bleu = bleu_metric.compute(predictions=decoded_predictions,references=[[r] for r in decoded_labels])\n",
    "\n",
    "    #Round values\n",
    "    rouge = {k: round(v, 4) for k, v in rouge.items()}\n",
    "    bleu = {k: [round(p, 4) for p in v ] if isinstance(v, list) else round(v, 4)  for k, v in bleu.items()}\n",
    "    results.update(rouge)\n",
    "    results.update((bleu))\n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Procedure:\n",
    "\n",
    "- The swin-gpt-2-image-captioning model is trained with the `Trainer` class from the transformers library.\n",
    "- The arguments used for training are set via `TrainingArguments` class.\n",
    "- The training is started by the `train ` method of the trainer class."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameters used by the `Trainer` for training and evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6825c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arg = TrainingArguments(\n",
    "    output_dir='../models/swin_image_captioning',  # dicts output\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=64,  # training batch size\n",
    "    per_device_eval_batch_size=64,  # evaluation batch size\n",
    "    load_best_model_at_end=True,\n",
    "    log_level='info',\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy='epoch', # set to 'steps' and the specify eval_steps=500,. This will give more data to plot\n",
    "    save_strategy='epoch',\n",
    "    use_mps_device=True, # use Apple Silicon\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arg,\n",
    "    compute_metrics=metrics,\n",
    "    train_dataset=processed_dataset['train'],\n",
    "    eval_dataset=processed_dataset['test'],\n",
    "    #data_collator=default_data_collator\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `VisionEncoderDecoderModel.forward` and have been ignored: coco_url, width, image_id, caption_id, file_name, height. If coco_url, width, image_id, caption_id, file_name, height are not expected by `VisionEncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::roll' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m()\n",
      "File \u001B[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/transformers/trainer.py:2774\u001B[0m, in \u001B[0;36mTrainer.evaluate\u001B[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001B[0m\n\u001B[1;32m   2771\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m   2773\u001B[0m eval_loop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprediction_loop \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39muse_legacy_prediction_loop \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluation_loop\n\u001B[0;32m-> 2774\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43meval_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2775\u001B[0m \u001B[43m    \u001B[49m\u001B[43meval_dataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2776\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdescription\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mEvaluation\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2777\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001B[39;49;00m\n\u001B[1;32m   2778\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# self.args.prediction_loss_only\u001B[39;49;00m\n\u001B[1;32m   2779\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprediction_loss_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_metrics\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   2780\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2781\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmetric_key_prefix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetric_key_prefix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2782\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2784\u001B[0m total_batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39meval_batch_size \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mworld_size\n\u001B[1;32m   2785\u001B[0m output\u001B[38;5;241m.\u001B[39mmetrics\u001B[38;5;241m.\u001B[39mupdate(\n\u001B[1;32m   2786\u001B[0m     speed_metrics(\n\u001B[1;32m   2787\u001B[0m         metric_key_prefix,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2791\u001B[0m     )\n\u001B[1;32m   2792\u001B[0m )\n",
      "File \u001B[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/transformers/trainer.py:2952\u001B[0m, in \u001B[0;36mTrainer.evaluation_loop\u001B[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001B[0m\n\u001B[1;32m   2949\u001B[0m         batch_size \u001B[38;5;241m=\u001B[39m observed_batch_size\n\u001B[1;32m   2951\u001B[0m \u001B[38;5;66;03m# Prediction step\u001B[39;00m\n\u001B[0;32m-> 2952\u001B[0m loss, logits, labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprediction_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprediction_loss_only\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2953\u001B[0m inputs_decode \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_input(inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m]) \u001B[38;5;28;01mif\u001B[39;00m args\u001B[38;5;241m.\u001B[39minclude_inputs_for_metrics \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2955\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_torch_tpu_available():\n",
      "File \u001B[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/transformers/trainer.py:3195\u001B[0m, in \u001B[0;36mTrainer.prediction_step\u001B[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001B[0m\n\u001B[1;32m   3193\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_labels:\n\u001B[1;32m   3194\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_context_manager():\n\u001B[0;32m-> 3195\u001B[0m         loss, outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m   3196\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mmean()\u001B[38;5;241m.\u001B[39mdetach()\n\u001B[1;32m   3198\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(outputs, \u001B[38;5;28mdict\u001B[39m):\n",
      "File \u001B[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/transformers/trainer.py:2518\u001B[0m, in \u001B[0;36mTrainer.compute_loss\u001B[0;34m(self, model, inputs, return_outputs)\u001B[0m\n\u001B[1;32m   2516\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2517\u001B[0m     labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 2518\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2519\u001B[0m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n\u001B[1;32m   2520\u001B[0m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n\u001B[1;32m   2521\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpast_index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1488\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1483\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1484\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1485\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1486\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1487\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1488\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1489\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1490\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py:469\u001B[0m, in \u001B[0;36mVisionEncoderDecoderModel.forward\u001B[0;34m(self, pixel_values, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001B[0m\n\u001B[1;32m    466\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m pixel_values \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    467\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou have to specify pixel_values\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 469\u001B[0m     encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    470\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpixel_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    471\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    472\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    473\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    474\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs_encoder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    475\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    476\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(encoder_outputs, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m    477\u001B[0m     encoder_outputs \u001B[38;5;241m=\u001B[39m BaseModelOutput(\u001B[38;5;241m*\u001B[39mencoder_outputs)\n",
      "File \u001B[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1488\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1483\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1484\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1485\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1486\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1487\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1488\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1489\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1490\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/transformers/models/swin/modeling_swin.py:978\u001B[0m, in \u001B[0;36mSwinModel.forward\u001B[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    974\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mdepths))\n\u001B[1;32m    976\u001B[0m embedding_output, input_dimensions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(pixel_values, bool_masked_pos\u001B[38;5;241m=\u001B[39mbool_masked_pos)\n\u001B[0;32m--> 978\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    979\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    980\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_dimensions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    981\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    982\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    983\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    984\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    985\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    987\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    988\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayernorm(sequence_output)\n",
      "File \u001B[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1488\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1483\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1484\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1485\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1486\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1487\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1488\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1489\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1490\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/transformers/models/swin/modeling_swin.py:820\u001B[0m, in \u001B[0;36mSwinEncoder.forward\u001B[0;34m(self, hidden_states, input_dimensions, head_mask, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    816\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mcheckpoint\u001B[38;5;241m.\u001B[39mcheckpoint(\n\u001B[1;32m    817\u001B[0m         create_custom_forward(layer_module), hidden_states, input_dimensions, layer_head_mask\n\u001B[1;32m    818\u001B[0m     )\n\u001B[1;32m    819\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 820\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_dimensions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    822\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    823\u001B[0m output_dimensions \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1488\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1483\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1484\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1485\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1486\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1487\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1488\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1489\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1490\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/transformers/models/swin/modeling_swin.py:742\u001B[0m, in \u001B[0;36mSwinStage.forward\u001B[0;34m(self, hidden_states, input_dimensions, head_mask, output_attentions)\u001B[0m\n\u001B[1;32m    738\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, layer_module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblocks):\n\u001B[1;32m    740\u001B[0m     layer_head_mask \u001B[38;5;241m=\u001B[39m head_mask[i] \u001B[38;5;28;01mif\u001B[39;00m head_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 742\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_dimensions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    744\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    746\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdownsample \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1488\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1483\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1484\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1485\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1486\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1487\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1488\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1489\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1490\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/transformers/models/swin/modeling_swin.py:662\u001B[0m, in \u001B[0;36mSwinLayer.forward\u001B[0;34m(self, hidden_states, input_dimensions, head_mask, output_attentions)\u001B[0m\n\u001B[1;32m    660\u001B[0m \u001B[38;5;66;03m# cyclic shift\u001B[39;00m\n\u001B[1;32m    661\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshift_size \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 662\u001B[0m     shifted_hidden_states \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mroll\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshifts\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshift_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshift_size\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdims\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    663\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    664\u001B[0m     shifted_hidden_states \u001B[38;5;241m=\u001B[39m hidden_states\n",
      "\u001B[0;31mNotImplementedError\u001B[0m: The operator 'aten::roll' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "\n",
    "trainer.evaluate()\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 4 epochs took 8 hours\n",
    "\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c1ef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#need to save the tokenizer\n",
    "tokenizer.save_pretrained('../models/swin_image_captioning')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc3b39e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# loading model and config from pretrained folder\n",
    "finetuned_model = VisionEncoderDecoderModel.from_pretrained('../models/swin_image_captioning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Make prediction using the pipeline library\n",
    "look into wheater you can pass the feature_extractor as a parameter as well.\n",
    "do_sample: returns multiple options\n",
    "top_p: getting more confident less random, sharpen predictions\n",
    "\n",
    "pretrained_generator = pipeline(\n",
    "    'text-generation', model=model, tokenizer='gpt2',\n",
    "    config={'max_length': 200, 'do_sample': True, 'top_p': 0.9, 'temperature': 0.7, 'top_k': 10}\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from IPython.core.display_functions import display\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inference_transforms = transforms.Compose(\n",
    "    [\n",
    "\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# a helper function to caption images from the web or a file path\n",
    "def generate_caption(m, path):\n",
    "\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    img_transformed = inference_transforms(img).unsqueeze(0)\n",
    "# tensor dimensions max_lenght X num_return_sequences, where ij == some_token_id\n",
    "    model_output = m.generate(\n",
    "        img_transformed,\n",
    "        num_beams=3,\n",
    "        max_length=15,\n",
    "        early_stopping=True,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=5,\n",
    "    )\n",
    "    # g is a tensor like this one: tensor([50256,    13,   198,   198,   198,   198,   198,   198,   198, 50256,\n",
    "    #50256, 50256, 50256, 50256, 50256])\n",
    "    captions = [tokenizer.decode(g, skip_special_tokens=True).strip() for g in model_output]\n",
    "    #Show image\n",
    "    display(img)\n",
    "    return captions, model_output, img_transformed\n",
    "\n",
    "\n",
    "captions, model_output, img_transformed = generate_caption(  # Out of sample photo\n",
    "    finetuned_model, '../data/test_data/000000421195_test.jpg'\n",
    ")\n",
    "\n",
    "captions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}